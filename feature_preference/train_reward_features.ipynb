{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3247ef59-99f4-47a9-99ba-61ab9d63e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import distributions as pyd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2b5ffe-ffe0-4ebf-8a35-1f805babc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "class RewardNetFeature(nn.Module):\n",
    "    def __init__(\n",
    "        self, feature_dim, output_mod=None):\n",
    "        super().__init__()\n",
    "        self.feature1 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 1)\n",
    "        )\n",
    "        self.feature2 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 1)\n",
    "        )\n",
    "        self.feature3 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 1)\n",
    "        )\n",
    "        self.feature4 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 1)\n",
    "        )\n",
    "        self.feature5 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 1)\n",
    "        )\n",
    "        self.feature6 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, feature_dim), nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 1)\n",
    "        )\n",
    "        self.pref_feature1 = nn.Linear(2,1)\n",
    "        self.pref_feature2 = nn.Linear(2,1)\n",
    "        self.pref_feature3 = nn.Linear(2,1)\n",
    "        self.pref_feature4 = nn.Linear(2,1)\n",
    "        self.pref_feature5 = nn.Linear(2,1)\n",
    "        self.pref_feature6 = nn.Linear(2,1)\n",
    "        self.pref = nn.Linear(6, 1)\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, feature1a, feature1b, feature2a, feature2b, feature3a, feature3b, \n",
    "                feature4a, feature4b, feature5a, feature5b, feature6a, feature6b, state1, state2):\n",
    "        feature1a = self.feature1(feature1a)\n",
    "        feature1b = self.feature1(feature1b)\n",
    "        feature2a = self.feature2(feature2a)\n",
    "        feature2b = self.feature2(feature2b)\n",
    "        feature3a = self.feature3(feature3a)\n",
    "        feature3b = self.feature3(feature3b)\n",
    "        feature4a = self.feature4(feature4a)\n",
    "        feature4b = self.feature4(feature4b)\n",
    "        feature5a = self.feature5(feature5a)\n",
    "        feature5b = self.feature5(feature5b)\n",
    "        feature6a = self.feature6(feature6a)\n",
    "        feature6b = self.feature6(feature6b)\n",
    "        pref_feature1 = self.pref_feature1(torch.squeeze(torch.stack([feature1a,feature1b], dim=1)))\n",
    "        pref_feature2 = self.pref_feature2(torch.squeeze(torch.stack([feature2a,feature2b], dim=1)))\n",
    "        pref_feature3 = self.pref_feature3(torch.squeeze(torch.stack([feature3a,feature3b], dim=1)))\n",
    "        pref_feature4 = self.pref_feature4(torch.squeeze(torch.stack([feature4a,feature4b], dim=1)))\n",
    "        pref_feature5 = self.pref_feature5(torch.squeeze(torch.stack([feature5a,feature5b], dim=1)))\n",
    "        pref_feature6 = self.pref_feature6(torch.squeeze(torch.stack([feature6a,feature6b], dim=1)))\n",
    "        pref = self.pref(torch.squeeze(torch.stack([pref_feature1, pref_feature2, pref_feature3, \n",
    "                                                    pref_feature4, pref_feature5, pref_feature6], dim=1)))\n",
    "        return pref_feature1, pref_feature2, pref_feature3, pref_feature4, pref_feature5, pref_feature6, pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c23618a1-3246-4c6d-877a-8209977f4488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardNetFeature(\n",
       "  (feature1): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
       "  )\n",
       "  (feature2): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
       "  )\n",
       "  (feature3): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
       "  )\n",
       "  (feature4): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
       "  )\n",
       "  (feature5): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
       "  )\n",
       "  (feature6): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
       "  )\n",
       "  (pref_feature1): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref_feature2): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref_feature3): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref_feature4): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref_feature5): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref_feature6): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dim = 3\n",
    "\n",
    "reward_net = RewardNetFeature(feature_dim)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "reward_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab01bacf-7a31-462a-b2d9-c857491ba2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data/train_rewards500.csv') as file_obj:\n",
    "    reader_obj = csv.reader(file_obj)\n",
    "\n",
    "    states1, states2, prefs = [], [], []\n",
    "    features1a, features1b, prefs_feature1 = [], [], []\n",
    "    features2a, features2b, prefs_feature2 = [], [], []\n",
    "    features3a, features3b, prefs_feature3 = [], [], []\n",
    "    features4a, features4b, prefs_feature4 = [], [], []\n",
    "    features5a, features5b, prefs_feature5 = [], [], []\n",
    "    features6a, features6b, prefs_feature6 = [], [], []\n",
    "    for row in reader_obj:\n",
    "        states1.append(row[0:18])\n",
    "        states2.append(row[19:37])\n",
    "        prefs.append(row[38])\n",
    "        features1a.append(row[0:3])\n",
    "        features1b.append(row[19:22])\n",
    "        prefs_feature1.append(row[39])\n",
    "        features2a.append(row[3:6])\n",
    "        features2b.append(row[22:25])\n",
    "        prefs_feature2.append(row[40])\n",
    "        features3a.append(row[6:9])\n",
    "        features3b.append(row[25:28])\n",
    "        prefs_feature3.append(row[41])\n",
    "        features4a.append(row[9:12])\n",
    "        features4b.append(row[28:31])\n",
    "        prefs_feature4.append(row[42])\n",
    "        features5a.append(row[12:15])\n",
    "        features5b.append(row[31:34])\n",
    "        prefs_feature5.append(row[43])\n",
    "        features6a.append(row[15:18])\n",
    "        features6b.append(row[34:37])\n",
    "        prefs_feature6.append(row[44])\n",
    "    states1 = np.array(states1,dtype=int)\n",
    "    states2 = np.array(states2,dtype=int)\n",
    "    prefs = np.array(prefs,dtype=int)\n",
    "    features1a = np.array(features1a,dtype=int)\n",
    "    features1b = np.array(features1b,dtype=int)\n",
    "    prefs_feature1 = np.array(prefs_feature1,dtype=int)\n",
    "    features2a = np.array(features2a,dtype=int)\n",
    "    features2b = np.array(features2b,dtype=int)\n",
    "    prefs_feature2 = np.array(prefs_feature2,dtype=int)\n",
    "    features3a = np.array(features3a,dtype=int)\n",
    "    features3b = np.array(features3b,dtype=int)\n",
    "    prefs_feature3 = np.array(prefs_feature3,dtype=int)\n",
    "    features4a = np.array(features4a,dtype=int)\n",
    "    features4b = np.array(features4b,dtype=int)\n",
    "    prefs_feature4 = np.array(prefs_feature4,dtype=int)\n",
    "    features5a = np.array(features5a,dtype=int)\n",
    "    features5b = np.array(features5b,dtype=int)\n",
    "    prefs_feature5 = np.array(prefs_feature5,dtype=int)\n",
    "    features6a = np.array(features6a,dtype=int)\n",
    "    features6b = np.array(features6b,dtype=int)\n",
    "    prefs_feature6 = np.array(prefs_feature6,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea49771b-bbf6-4f0e-b0d7-4c7acdea7211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 5.04530048\n",
      "[2,     1] loss: 4.96073055\n",
      "[3,     1] loss: 4.88322639\n",
      "[4,     1] loss: 4.81454468\n",
      "[5,     1] loss: 4.75017309\n",
      "[6,     1] loss: 4.69121695\n",
      "[7,     1] loss: 4.63620377\n",
      "[8,     1] loss: 4.58269835\n",
      "[9,     1] loss: 4.53004169\n",
      "[10,     1] loss: 4.47978687\n",
      "[11,     1] loss: 4.42974043\n",
      "[12,     1] loss: 4.37910032\n",
      "[13,     1] loss: 4.32768774\n",
      "[14,     1] loss: 4.27522182\n",
      "[15,     1] loss: 4.22077703\n",
      "[16,     1] loss: 4.16499519\n",
      "[17,     1] loss: 4.10817623\n",
      "[18,     1] loss: 4.05134487\n",
      "[19,     1] loss: 3.99502945\n",
      "[20,     1] loss: 3.93775320\n",
      "[21,     1] loss: 3.88023567\n",
      "[22,     1] loss: 3.82288718\n",
      "[23,     1] loss: 3.76586485\n",
      "[24,     1] loss: 3.70976591\n",
      "[25,     1] loss: 3.65492868\n",
      "[26,     1] loss: 3.60109758\n",
      "[27,     1] loss: 3.54840660\n",
      "[28,     1] loss: 3.49697971\n",
      "[29,     1] loss: 3.44704103\n",
      "[30,     1] loss: 3.39841986\n",
      "[31,     1] loss: 3.35102296\n",
      "[32,     1] loss: 3.30478001\n",
      "[33,     1] loss: 3.25967383\n",
      "[34,     1] loss: 3.21560717\n",
      "[35,     1] loss: 3.17243123\n",
      "[36,     1] loss: 3.13005018\n",
      "[37,     1] loss: 3.08836937\n",
      "[38,     1] loss: 3.04718208\n",
      "[39,     1] loss: 3.00645185\n",
      "[40,     1] loss: 2.96601629\n",
      "[41,     1] loss: 2.92582369\n",
      "[42,     1] loss: 2.88582802\n",
      "[43,     1] loss: 2.84595585\n",
      "[44,     1] loss: 2.80617929\n",
      "[45,     1] loss: 2.76645756\n",
      "[46,     1] loss: 2.72678471\n",
      "[47,     1] loss: 2.68716216\n",
      "[48,     1] loss: 2.64755869\n",
      "[49,     1] loss: 2.60797644\n",
      "[50,     1] loss: 2.56837988\n",
      "[51,     1] loss: 2.52833796\n",
      "[52,     1] loss: 2.48817301\n",
      "[53,     1] loss: 2.44805717\n",
      "[54,     1] loss: 2.40799141\n",
      "[55,     1] loss: 2.36791778\n",
      "[56,     1] loss: 2.32796907\n",
      "[57,     1] loss: 2.28783083\n",
      "[58,     1] loss: 2.24734855\n",
      "[59,     1] loss: 2.20457625\n",
      "[60,     1] loss: 2.16051912\n",
      "[61,     1] loss: 2.11559606\n",
      "[62,     1] loss: 2.06976986\n",
      "[63,     1] loss: 2.02359796\n",
      "[64,     1] loss: 1.97740209\n",
      "[65,     1] loss: 1.93147218\n",
      "[66,     1] loss: 1.88617015\n",
      "[67,     1] loss: 1.84147012\n",
      "[68,     1] loss: 1.79773748\n",
      "[69,     1] loss: 1.75458014\n",
      "[70,     1] loss: 1.71234870\n",
      "[71,     1] loss: 1.67045522\n",
      "[72,     1] loss: 1.62897766\n",
      "[73,     1] loss: 1.58802843\n",
      "[74,     1] loss: 1.54713953\n",
      "[75,     1] loss: 1.50664592\n",
      "[76,     1] loss: 1.46603906\n",
      "[77,     1] loss: 1.42573535\n",
      "[78,     1] loss: 1.38547683\n",
      "[79,     1] loss: 1.34543669\n",
      "[80,     1] loss: 1.30564713\n",
      "[81,     1] loss: 1.26609123\n",
      "[82,     1] loss: 1.22714996\n",
      "[83,     1] loss: 1.18864977\n",
      "[84,     1] loss: 1.15089154\n",
      "[85,     1] loss: 1.11377335\n",
      "[86,     1] loss: 1.07737684\n",
      "[87,     1] loss: 1.04182696\n",
      "[88,     1] loss: 1.00689590\n",
      "[89,     1] loss: 0.97273111\n",
      "[90,     1] loss: 0.93916214\n",
      "[91,     1] loss: 0.90624923\n",
      "[92,     1] loss: 0.87382907\n",
      "[93,     1] loss: 0.84206742\n",
      "[94,     1] loss: 0.81110162\n",
      "[95,     1] loss: 0.78091443\n",
      "[96,     1] loss: 0.75174963\n",
      "[97,     1] loss: 0.72362477\n",
      "[98,     1] loss: 0.69679403\n",
      "[99,     1] loss: 0.67146248\n",
      "[100,     1] loss: 0.64761621\n",
      "[101,     1] loss: 0.62527561\n",
      "[102,     1] loss: 0.60457689\n",
      "[103,     1] loss: 0.58522284\n",
      "[104,     1] loss: 0.56721056\n",
      "[105,     1] loss: 0.55032003\n",
      "[106,     1] loss: 0.53437638\n",
      "[107,     1] loss: 0.51939559\n",
      "[108,     1] loss: 0.50512451\n",
      "[109,     1] loss: 0.49154350\n",
      "[110,     1] loss: 0.47862560\n",
      "[111,     1] loss: 0.46622357\n",
      "[112,     1] loss: 0.45440105\n",
      "[113,     1] loss: 0.44305131\n",
      "[114,     1] loss: 0.43213445\n",
      "[115,     1] loss: 0.42172521\n",
      "[116,     1] loss: 0.41168135\n",
      "[117,     1] loss: 0.40163395\n",
      "[118,     1] loss: 0.39119592\n",
      "[119,     1] loss: 0.38107911\n",
      "[120,     1] loss: 0.37137091\n",
      "[121,     1] loss: 0.36199254\n",
      "[122,     1] loss: 0.35295701\n",
      "[123,     1] loss: 0.34425107\n",
      "[124,     1] loss: 0.33584553\n",
      "[125,     1] loss: 0.32778171\n",
      "[126,     1] loss: 0.31998029\n",
      "[127,     1] loss: 0.31246781\n",
      "[128,     1] loss: 0.30521750\n",
      "[129,     1] loss: 0.29820621\n",
      "[130,     1] loss: 0.29145586\n",
      "[131,     1] loss: 0.28490883\n",
      "[132,     1] loss: 0.27859670\n",
      "[133,     1] loss: 0.27248353\n",
      "[134,     1] loss: 0.26656970\n",
      "[135,     1] loss: 0.26085079\n",
      "[136,     1] loss: 0.25529855\n",
      "[137,     1] loss: 0.24994193\n",
      "[138,     1] loss: 0.24473393\n",
      "[139,     1] loss: 0.23969518\n",
      "[140,     1] loss: 0.23480533\n",
      "[141,     1] loss: 0.23006457\n",
      "[142,     1] loss: 0.22546344\n",
      "[143,     1] loss: 0.22099702\n",
      "[144,     1] loss: 0.21665703\n",
      "[145,     1] loss: 0.21243627\n",
      "[146,     1] loss: 0.20834914\n",
      "[147,     1] loss: 0.20436354\n",
      "[148,     1] loss: 0.20049666\n",
      "[149,     1] loss: 0.19673038\n",
      "[150,     1] loss: 0.19306883\n",
      "[151,     1] loss: 0.18950520\n",
      "[152,     1] loss: 0.18603531\n",
      "[153,     1] loss: 0.18265867\n",
      "[154,     1] loss: 0.17936420\n",
      "[155,     1] loss: 0.17616260\n",
      "[156,     1] loss: 0.17303534\n",
      "[157,     1] loss: 0.16999431\n",
      "[158,     1] loss: 0.16702528\n",
      "[159,     1] loss: 0.16412991\n",
      "[160,     1] loss: 0.16130313\n",
      "[161,     1] loss: 0.15855014\n",
      "[162,     1] loss: 0.15585719\n",
      "[163,     1] loss: 0.15323612\n",
      "[164,     1] loss: 0.15066947\n",
      "[165,     1] loss: 0.14817022\n",
      "[166,     1] loss: 0.14572409\n",
      "[167,     1] loss: 0.14333904\n",
      "[168,     1] loss: 0.14101152\n",
      "[169,     1] loss: 0.13872515\n",
      "[170,     1] loss: 0.13649563\n",
      "[171,     1] loss: 0.13431826\n",
      "[172,     1] loss: 0.13218758\n",
      "[173,     1] loss: 0.13010435\n",
      "[174,     1] loss: 0.12806658\n",
      "[175,     1] loss: 0.12607270\n",
      "[176,     1] loss: 0.12412313\n",
      "[177,     1] loss: 0.12221299\n",
      "[178,     1] loss: 0.12034640\n",
      "[179,     1] loss: 0.11852109\n",
      "[180,     1] loss: 0.11672704\n",
      "[181,     1] loss: 0.11497371\n",
      "[182,     1] loss: 0.11325669\n",
      "[183,     1] loss: 0.11157520\n",
      "[184,     1] loss: 0.10992683\n",
      "[185,     1] loss: 0.10831446\n",
      "[186,     1] loss: 0.10673153\n",
      "[187,     1] loss: 0.10518263\n",
      "[188,     1] loss: 0.10366461\n",
      "[189,     1] loss: 0.10217445\n",
      "[190,     1] loss: 0.10071585\n",
      "[191,     1] loss: 0.09928380\n",
      "[192,     1] loss: 0.09788034\n",
      "[193,     1] loss: 0.09650506\n",
      "[194,     1] loss: 0.09515374\n",
      "[195,     1] loss: 0.09382983\n",
      "[196,     1] loss: 0.09253093\n",
      "[197,     1] loss: 0.09125539\n",
      "[198,     1] loss: 0.09000473\n",
      "[199,     1] loss: 0.08877718\n",
      "[200,     1] loss: 0.08757147\n",
      "[201,     1] loss: 0.08638882\n",
      "[202,     1] loss: 0.08522792\n",
      "[203,     1] loss: 0.08408766\n",
      "[204,     1] loss: 0.08296766\n",
      "[205,     1] loss: 0.08186838\n",
      "[206,     1] loss: 0.08078885\n",
      "[207,     1] loss: 0.07972766\n",
      "[208,     1] loss: 0.07868592\n",
      "[209,     1] loss: 0.07766278\n",
      "[210,     1] loss: 0.07665721\n",
      "[211,     1] loss: 0.07566855\n",
      "[212,     1] loss: 0.07469742\n",
      "[213,     1] loss: 0.07374354\n",
      "[214,     1] loss: 0.07280561\n",
      "[215,     1] loss: 0.07188368\n",
      "[216,     1] loss: 0.07097714\n",
      "[217,     1] loss: 0.07008597\n",
      "[218,     1] loss: 0.06920984\n",
      "[219,     1] loss: 0.06834836\n",
      "[220,     1] loss: 0.06750135\n",
      "[221,     1] loss: 0.06666806\n",
      "[222,     1] loss: 0.06584862\n",
      "[223,     1] loss: 0.06504246\n",
      "[224,     1] loss: 0.06424959\n",
      "[225,     1] loss: 0.06346919\n",
      "[226,     1] loss: 0.06270196\n",
      "[227,     1] loss: 0.06194660\n",
      "[228,     1] loss: 0.06120347\n",
      "[229,     1] loss: 0.06047222\n",
      "[230,     1] loss: 0.05975254\n",
      "[231,     1] loss: 0.05904417\n",
      "[232,     1] loss: 0.05834692\n",
      "[233,     1] loss: 0.05766041\n",
      "[234,     1] loss: 0.05698469\n",
      "[235,     1] loss: 0.05631955\n",
      "[236,     1] loss: 0.05566443\n",
      "[237,     1] loss: 0.05501938\n",
      "[238,     1] loss: 0.05438409\n",
      "[239,     1] loss: 0.05375895\n",
      "[240,     1] loss: 0.05314251\n",
      "[241,     1] loss: 0.05253587\n",
      "[242,     1] loss: 0.05193799\n",
      "[243,     1] loss: 0.05134926\n",
      "[244,     1] loss: 0.05076913\n",
      "[245,     1] loss: 0.05019775\n",
      "[246,     1] loss: 0.04963486\n",
      "[247,     1] loss: 0.04907983\n",
      "[248,     1] loss: 0.04853311\n",
      "[249,     1] loss: 0.04799439\n",
      "[250,     1] loss: 0.04746352\n",
      "[251,     1] loss: 0.04694044\n",
      "[252,     1] loss: 0.04642456\n",
      "[253,     1] loss: 0.04591622\n",
      "[254,     1] loss: 0.04541487\n",
      "[255,     1] loss: 0.04492087\n",
      "[256,     1] loss: 0.04443379\n",
      "[257,     1] loss: 0.04394931\n",
      "[258,     1] loss: 0.04344939\n",
      "[259,     1] loss: 0.04295189\n",
      "[260,     1] loss: 0.04246246\n",
      "[261,     1] loss: 0.04198117\n",
      "[262,     1] loss: 0.04150835\n",
      "[263,     1] loss: 0.04104337\n",
      "[264,     1] loss: 0.04058668\n",
      "[265,     1] loss: 0.04013740\n",
      "[266,     1] loss: 0.03969535\n",
      "[267,     1] loss: 0.03926055\n",
      "[268,     1] loss: 0.03883230\n",
      "[269,     1] loss: 0.03841089\n",
      "[270,     1] loss: 0.03799581\n",
      "[271,     1] loss: 0.03758676\n",
      "[272,     1] loss: 0.03718398\n",
      "[273,     1] loss: 0.03678712\n",
      "[274,     1] loss: 0.03639584\n",
      "[275,     1] loss: 0.03601027\n",
      "[276,     1] loss: 0.03563022\n",
      "[277,     1] loss: 0.03525551\n",
      "[278,     1] loss: 0.03488621\n",
      "[279,     1] loss: 0.03452209\n",
      "[280,     1] loss: 0.03416278\n",
      "[281,     1] loss: 0.03380867\n",
      "[282,     1] loss: 0.03345940\n",
      "[283,     1] loss: 0.03311485\n",
      "[284,     1] loss: 0.03277496\n",
      "[285,     1] loss: 0.03243978\n",
      "[286,     1] loss: 0.03210891\n",
      "[287,     1] loss: 0.03178266\n",
      "[288,     1] loss: 0.03146065\n",
      "[289,     1] loss: 0.03114298\n",
      "[290,     1] loss: 0.03082948\n",
      "[291,     1] loss: 0.03051049\n",
      "[292,     1] loss: 0.03016189\n",
      "[293,     1] loss: 0.02980688\n",
      "[294,     1] loss: 0.02945883\n",
      "[295,     1] loss: 0.02911836\n",
      "[296,     1] loss: 0.02878536\n",
      "[297,     1] loss: 0.02845952\n",
      "[298,     1] loss: 0.02814028\n",
      "[299,     1] loss: 0.02782722\n",
      "[300,     1] loss: 0.02751993\n",
      "[301,     1] loss: 0.02721816\n",
      "[302,     1] loss: 0.02692191\n",
      "[303,     1] loss: 0.02663064\n",
      "[304,     1] loss: 0.02634424\n",
      "[305,     1] loss: 0.02606260\n",
      "[306,     1] loss: 0.02578560\n",
      "[307,     1] loss: 0.02551308\n",
      "[308,     1] loss: 0.02524502\n",
      "[309,     1] loss: 0.02498094\n",
      "[310,     1] loss: 0.02472115\n",
      "[311,     1] loss: 0.02446520\n",
      "[312,     1] loss: 0.02421325\n",
      "[313,     1] loss: 0.02396502\n",
      "[314,     1] loss: 0.02372059\n",
      "[315,     1] loss: 0.02347969\n",
      "[316,     1] loss: 0.02324229\n",
      "[317,     1] loss: 0.02300842\n",
      "[318,     1] loss: 0.02277786\n",
      "[319,     1] loss: 0.02255054\n",
      "[320,     1] loss: 0.02232644\n",
      "[321,     1] loss: 0.02210554\n",
      "[322,     1] loss: 0.02188770\n",
      "[323,     1] loss: 0.02167271\n",
      "[324,     1] loss: 0.02146077\n",
      "[325,     1] loss: 0.02125174\n",
      "[326,     1] loss: 0.02104550\n",
      "[327,     1] loss: 0.02084193\n",
      "[328,     1] loss: 0.02064103\n",
      "[329,     1] loss: 0.02044285\n",
      "[330,     1] loss: 0.02024732\n",
      "[331,     1] loss: 0.02005418\n",
      "[332,     1] loss: 0.01986383\n",
      "[333,     1] loss: 0.01967588\n",
      "[334,     1] loss: 0.01949013\n",
      "[335,     1] loss: 0.01930677\n",
      "[336,     1] loss: 0.01912558\n",
      "[337,     1] loss: 0.01894670\n",
      "[338,     1] loss: 0.01877008\n",
      "[339,     1] loss: 0.01859574\n",
      "[340,     1] loss: 0.01842334\n",
      "[341,     1] loss: 0.01825318\n",
      "[342,     1] loss: 0.01808509\n",
      "[343,     1] loss: 0.01791896\n",
      "[344,     1] loss: 0.01775467\n",
      "[345,     1] loss: 0.01759260\n",
      "[346,     1] loss: 0.01743228\n",
      "[347,     1] loss: 0.01727399\n",
      "[348,     1] loss: 0.01711755\n",
      "[349,     1] loss: 0.01696283\n",
      "[350,     1] loss: 0.01680987\n",
      "[351,     1] loss: 0.01665888\n",
      "[352,     1] loss: 0.01650942\n",
      "[353,     1] loss: 0.01636180\n",
      "[354,     1] loss: 0.01621577\n",
      "[355,     1] loss: 0.01607148\n",
      "[356,     1] loss: 0.01592880\n",
      "[357,     1] loss: 0.01578771\n",
      "[358,     1] loss: 0.01564820\n",
      "[359,     1] loss: 0.01551030\n",
      "[360,     1] loss: 0.01537385\n",
      "[361,     1] loss: 0.01523905\n",
      "[362,     1] loss: 0.01510556\n",
      "[363,     1] loss: 0.01497362\n",
      "[364,     1] loss: 0.01484311\n",
      "[365,     1] loss: 0.01471406\n",
      "[366,     1] loss: 0.01458630\n",
      "[367,     1] loss: 0.01446003\n",
      "[368,     1] loss: 0.01433517\n",
      "[369,     1] loss: 0.01421155\n",
      "[370,     1] loss: 0.01408922\n",
      "[371,     1] loss: 0.01396821\n",
      "[372,     1] loss: 0.01384864\n",
      "[373,     1] loss: 0.01373012\n",
      "[374,     1] loss: 0.01361286\n",
      "[375,     1] loss: 0.01349701\n",
      "[376,     1] loss: 0.01338232\n",
      "[377,     1] loss: 0.01326866\n",
      "[378,     1] loss: 0.01315635\n",
      "[379,     1] loss: 0.01304522\n",
      "[380,     1] loss: 0.01293507\n",
      "[381,     1] loss: 0.01282620\n",
      "[382,     1] loss: 0.01271842\n",
      "[383,     1] loss: 0.01261168\n",
      "[384,     1] loss: 0.01250607\n",
      "[385,     1] loss: 0.01240152\n",
      "[386,     1] loss: 0.01229812\n",
      "[387,     1] loss: 0.01219558\n",
      "[388,     1] loss: 0.01209421\n",
      "[389,     1] loss: 0.01199392\n",
      "[390,     1] loss: 0.01189456\n",
      "[391,     1] loss: 0.01179619\n",
      "[392,     1] loss: 0.01169868\n",
      "[393,     1] loss: 0.01160226\n",
      "[394,     1] loss: 0.01150675\n",
      "[395,     1] loss: 0.01141226\n",
      "[396,     1] loss: 0.01131865\n",
      "[397,     1] loss: 0.01122594\n",
      "[398,     1] loss: 0.01113419\n",
      "[399,     1] loss: 0.01104322\n",
      "[400,     1] loss: 0.01095329\n",
      "[401,     1] loss: 0.01086417\n",
      "[402,     1] loss: 0.01077586\n",
      "[403,     1] loss: 0.01068855\n",
      "[404,     1] loss: 0.01060192\n",
      "[405,     1] loss: 0.01051614\n",
      "[406,     1] loss: 0.01043127\n",
      "[407,     1] loss: 0.01034718\n",
      "[408,     1] loss: 0.01026396\n",
      "[409,     1] loss: 0.01018143\n",
      "[410,     1] loss: 0.01009973\n",
      "[411,     1] loss: 0.01001874\n",
      "[412,     1] loss: 0.00993850\n",
      "[413,     1] loss: 0.00985909\n",
      "[414,     1] loss: 0.00978043\n",
      "[415,     1] loss: 0.00970245\n",
      "[416,     1] loss: 0.00962528\n",
      "[417,     1] loss: 0.00954880\n",
      "[418,     1] loss: 0.00947302\n",
      "[419,     1] loss: 0.00939793\n",
      "[420,     1] loss: 0.00932365\n",
      "[421,     1] loss: 0.00924994\n",
      "[422,     1] loss: 0.00917693\n",
      "[423,     1] loss: 0.00910455\n",
      "[424,     1] loss: 0.00903294\n",
      "[425,     1] loss: 0.00896194\n",
      "[426,     1] loss: 0.00889154\n",
      "[427,     1] loss: 0.00882187\n",
      "[428,     1] loss: 0.00875273\n",
      "[429,     1] loss: 0.00868425\n",
      "[430,     1] loss: 0.00861651\n",
      "[431,     1] loss: 0.00854930\n",
      "[432,     1] loss: 0.00848269\n",
      "[433,     1] loss: 0.00841672\n",
      "[434,     1] loss: 0.00835132\n",
      "[435,     1] loss: 0.00828644\n",
      "[436,     1] loss: 0.00822221\n",
      "[437,     1] loss: 0.00815865\n",
      "[438,     1] loss: 0.00809560\n",
      "[439,     1] loss: 0.00803298\n",
      "[440,     1] loss: 0.00797102\n",
      "[441,     1] loss: 0.00790966\n",
      "[442,     1] loss: 0.00784879\n",
      "[443,     1] loss: 0.00778841\n",
      "[444,     1] loss: 0.00772859\n",
      "[445,     1] loss: 0.00766939\n",
      "[446,     1] loss: 0.00761061\n",
      "[447,     1] loss: 0.00755234\n",
      "[448,     1] loss: 0.00749458\n",
      "[449,     1] loss: 0.00743735\n",
      "[450,     1] loss: 0.00738062\n",
      "[451,     1] loss: 0.00732449\n",
      "[452,     1] loss: 0.00726882\n",
      "[453,     1] loss: 0.00721352\n",
      "[454,     1] loss: 0.00715878\n",
      "[455,     1] loss: 0.00710445\n",
      "[456,     1] loss: 0.00705058\n",
      "[457,     1] loss: 0.00699723\n",
      "[458,     1] loss: 0.00694436\n",
      "[459,     1] loss: 0.00689195\n",
      "[460,     1] loss: 0.00683992\n",
      "[461,     1] loss: 0.00678844\n",
      "[462,     1] loss: 0.00673730\n",
      "[463,     1] loss: 0.00668668\n",
      "[464,     1] loss: 0.00663643\n",
      "[465,     1] loss: 0.00658658\n",
      "[466,     1] loss: 0.00653723\n",
      "[467,     1] loss: 0.00648827\n",
      "[468,     1] loss: 0.00643967\n",
      "[469,     1] loss: 0.00639153\n",
      "[470,     1] loss: 0.00634386\n",
      "[471,     1] loss: 0.00629656\n",
      "[472,     1] loss: 0.00624959\n",
      "[473,     1] loss: 0.00620304\n",
      "[474,     1] loss: 0.00615693\n",
      "[475,     1] loss: 0.00611115\n",
      "[476,     1] loss: 0.00606585\n",
      "[477,     1] loss: 0.00602085\n",
      "[478,     1] loss: 0.00597622\n",
      "[479,     1] loss: 0.00593200\n",
      "[480,     1] loss: 0.00588811\n",
      "[481,     1] loss: 0.00584461\n",
      "[482,     1] loss: 0.00580150\n",
      "[483,     1] loss: 0.00575867\n",
      "[484,     1] loss: 0.00571624\n",
      "[485,     1] loss: 0.00567423\n",
      "[486,     1] loss: 0.00563246\n",
      "[487,     1] loss: 0.00559106\n",
      "[488,     1] loss: 0.00555000\n",
      "[489,     1] loss: 0.00550932\n",
      "[490,     1] loss: 0.00546893\n",
      "[491,     1] loss: 0.00542887\n",
      "[492,     1] loss: 0.00538923\n",
      "[493,     1] loss: 0.00534977\n",
      "[494,     1] loss: 0.00531077\n",
      "[495,     1] loss: 0.00527201\n",
      "[496,     1] loss: 0.00523355\n",
      "[497,     1] loss: 0.00519537\n",
      "[498,     1] loss: 0.00515765\n",
      "[499,     1] loss: 0.00512011\n",
      "[500,     1] loss: 0.00508282\n",
      "[501,     1] loss: 0.00504600\n",
      "[502,     1] loss: 0.00500939\n",
      "[503,     1] loss: 0.00497310\n",
      "[504,     1] loss: 0.00493712\n",
      "[505,     1] loss: 0.00490147\n",
      "[506,     1] loss: 0.00486599\n",
      "[507,     1] loss: 0.00483078\n",
      "[508,     1] loss: 0.00479599\n",
      "[509,     1] loss: 0.00476128\n",
      "[510,     1] loss: 0.00472708\n",
      "[511,     1] loss: 0.00469301\n",
      "[512,     1] loss: 0.00465928\n",
      "[513,     1] loss: 0.00462569\n",
      "[514,     1] loss: 0.00459255\n",
      "[515,     1] loss: 0.00455951\n",
      "[516,     1] loss: 0.00452689\n",
      "[517,     1] loss: 0.00449445\n",
      "[518,     1] loss: 0.00446224\n",
      "[519,     1] loss: 0.00443027\n",
      "[520,     1] loss: 0.00439861\n",
      "[521,     1] loss: 0.00436717\n",
      "[522,     1] loss: 0.00433602\n",
      "[523,     1] loss: 0.00430497\n",
      "[524,     1] loss: 0.00427439\n",
      "[525,     1] loss: 0.00424392\n",
      "[526,     1] loss: 0.00421374\n",
      "[527,     1] loss: 0.00418371\n",
      "[528,     1] loss: 0.00415402\n",
      "[529,     1] loss: 0.00412452\n",
      "[530,     1] loss: 0.00409529\n",
      "[531,     1] loss: 0.00406619\n",
      "[532,     1] loss: 0.00403731\n",
      "[533,     1] loss: 0.00400877\n",
      "[534,     1] loss: 0.00398040\n",
      "[535,     1] loss: 0.00395227\n",
      "[536,     1] loss: 0.00392427\n",
      "[537,     1] loss: 0.00389664\n",
      "[538,     1] loss: 0.00386905\n",
      "[539,     1] loss: 0.00384178\n",
      "[540,     1] loss: 0.00381471\n",
      "[541,     1] loss: 0.00378790\n",
      "[542,     1] loss: 0.00376116\n",
      "[543,     1] loss: 0.00373477\n",
      "[544,     1] loss: 0.00370851\n",
      "[545,     1] loss: 0.00368244\n",
      "[546,     1] loss: 0.00365655\n",
      "[547,     1] loss: 0.00363094\n",
      "[548,     1] loss: 0.00360553\n",
      "[549,     1] loss: 0.00358030\n",
      "[550,     1] loss: 0.00355519\n",
      "[551,     1] loss: 0.00353029\n",
      "[552,     1] loss: 0.00350559\n",
      "[553,     1] loss: 0.00348114\n",
      "[554,     1] loss: 0.00345685\n",
      "[555,     1] loss: 0.00343273\n",
      "[556,     1] loss: 0.00340876\n",
      "[557,     1] loss: 0.00338494\n",
      "[558,     1] loss: 0.00336149\n",
      "[559,     1] loss: 0.00333809\n",
      "[560,     1] loss: 0.00331487\n",
      "[561,     1] loss: 0.00329179\n",
      "[562,     1] loss: 0.00326895\n",
      "[563,     1] loss: 0.00324616\n",
      "[564,     1] loss: 0.00322372\n",
      "[565,     1] loss: 0.00320139\n",
      "[566,     1] loss: 0.00317907\n",
      "[567,     1] loss: 0.00315716\n",
      "[568,     1] loss: 0.00313526\n",
      "[569,     1] loss: 0.00311359\n",
      "[570,     1] loss: 0.00309210\n",
      "[571,     1] loss: 0.00307067\n",
      "[572,     1] loss: 0.00304945\n",
      "[573,     1] loss: 0.00302839\n",
      "[574,     1] loss: 0.00300749\n",
      "[575,     1] loss: 0.00298683\n",
      "[576,     1] loss: 0.00296623\n",
      "[577,     1] loss: 0.00294582\n",
      "[578,     1] loss: 0.00292556\n",
      "[579,     1] loss: 0.00290544\n",
      "[580,     1] loss: 0.00288547\n",
      "[581,     1] loss: 0.00286559\n",
      "[582,     1] loss: 0.00284589\n",
      "[583,     1] loss: 0.00282643\n",
      "[584,     1] loss: 0.00280711\n",
      "[585,     1] loss: 0.00278772\n",
      "[586,     1] loss: 0.00276867\n",
      "[587,     1] loss: 0.00274971\n",
      "[588,     1] loss: 0.00273096\n",
      "[589,     1] loss: 0.00271226\n",
      "[590,     1] loss: 0.00269366\n",
      "[591,     1] loss: 0.00267530\n",
      "[592,     1] loss: 0.00265703\n",
      "[593,     1] loss: 0.00263895\n",
      "[594,     1] loss: 0.00262089\n",
      "[595,     1] loss: 0.00260304\n",
      "[596,     1] loss: 0.00258524\n",
      "[597,     1] loss: 0.00256774\n",
      "[598,     1] loss: 0.00255017\n",
      "[599,     1] loss: 0.00253289\n",
      "[600,     1] loss: 0.00251565\n",
      "[601,     1] loss: 0.00249858\n",
      "[602,     1] loss: 0.00248161\n",
      "[603,     1] loss: 0.00246474\n",
      "[604,     1] loss: 0.00244794\n",
      "[605,     1] loss: 0.00243139\n",
      "[606,     1] loss: 0.00241492\n",
      "[607,     1] loss: 0.00239856\n",
      "[608,     1] loss: 0.00238236\n",
      "[609,     1] loss: 0.00236610\n",
      "[610,     1] loss: 0.00235008\n",
      "[611,     1] loss: 0.00233431\n",
      "[612,     1] loss: 0.00231850\n",
      "[613,     1] loss: 0.00230283\n",
      "[614,     1] loss: 0.00228731\n",
      "[615,     1] loss: 0.00227182\n",
      "[616,     1] loss: 0.00225653\n",
      "[617,     1] loss: 0.00224133\n",
      "[618,     1] loss: 0.00222627\n",
      "[619,     1] loss: 0.00221127\n",
      "[620,     1] loss: 0.00219635\n",
      "[621,     1] loss: 0.00218157\n",
      "[622,     1] loss: 0.00216690\n",
      "[623,     1] loss: 0.00215234\n",
      "[624,     1] loss: 0.00213781\n",
      "[625,     1] loss: 0.00212349\n",
      "[626,     1] loss: 0.00210926\n",
      "[627,     1] loss: 0.00209512\n",
      "[628,     1] loss: 0.00208108\n",
      "[629,     1] loss: 0.00206712\n",
      "[630,     1] loss: 0.00205324\n",
      "[631,     1] loss: 0.00203948\n",
      "[632,     1] loss: 0.00202589\n",
      "[633,     1] loss: 0.00201226\n",
      "[634,     1] loss: 0.00199886\n",
      "[635,     1] loss: 0.00198543\n",
      "[636,     1] loss: 0.00197224\n",
      "[637,     1] loss: 0.00195907\n",
      "[638,     1] loss: 0.00194595\n",
      "[639,     1] loss: 0.00193296\n",
      "[640,     1] loss: 0.00192007\n",
      "[641,     1] loss: 0.00190728\n",
      "[642,     1] loss: 0.00189462\n",
      "[643,     1] loss: 0.00188195\n",
      "[644,     1] loss: 0.00186946\n",
      "[645,     1] loss: 0.00185697\n",
      "[646,     1] loss: 0.00184463\n",
      "[647,     1] loss: 0.00183239\n",
      "[648,     1] loss: 0.00182012\n",
      "[649,     1] loss: 0.00180805\n",
      "[650,     1] loss: 0.00179606\n",
      "[651,     1] loss: 0.00178409\n",
      "[652,     1] loss: 0.00177238\n",
      "[653,     1] loss: 0.00176060\n",
      "[654,     1] loss: 0.00174887\n",
      "[655,     1] loss: 0.00173733\n",
      "[656,     1] loss: 0.00172579\n",
      "[657,     1] loss: 0.00171438\n",
      "[658,     1] loss: 0.00170305\n",
      "[659,     1] loss: 0.00169170\n",
      "[660,     1] loss: 0.00168051\n",
      "[661,     1] loss: 0.00166942\n",
      "[662,     1] loss: 0.00165836\n",
      "[663,     1] loss: 0.00164740\n",
      "[664,     1] loss: 0.00163662\n",
      "[665,     1] loss: 0.00162584\n",
      "[666,     1] loss: 0.00161503\n",
      "[667,     1] loss: 0.00160449\n",
      "[668,     1] loss: 0.00159384\n",
      "[669,     1] loss: 0.00158341\n",
      "[670,     1] loss: 0.00157291\n",
      "[671,     1] loss: 0.00156255\n",
      "[672,     1] loss: 0.00155232\n",
      "[673,     1] loss: 0.00154212\n",
      "[674,     1] loss: 0.00153190\n",
      "[675,     1] loss: 0.00152178\n",
      "[676,     1] loss: 0.00151188\n",
      "[677,     1] loss: 0.00150195\n",
      "[678,     1] loss: 0.00149205\n",
      "[679,     1] loss: 0.00148224\n",
      "[680,     1] loss: 0.00147259\n",
      "[681,     1] loss: 0.00146287\n",
      "[682,     1] loss: 0.00145319\n",
      "[683,     1] loss: 0.00144375\n",
      "[684,     1] loss: 0.00143434\n",
      "[685,     1] loss: 0.00142487\n",
      "[686,     1] loss: 0.00141556\n",
      "[687,     1] loss: 0.00140633\n",
      "[688,     1] loss: 0.00139715\n",
      "[689,     1] loss: 0.00138799\n",
      "[690,     1] loss: 0.00137893\n",
      "[691,     1] loss: 0.00136992\n",
      "[692,     1] loss: 0.00136094\n",
      "[693,     1] loss: 0.00135200\n",
      "[694,     1] loss: 0.00134317\n",
      "[695,     1] loss: 0.00133448\n",
      "[696,     1] loss: 0.00132573\n",
      "[697,     1] loss: 0.00131714\n",
      "[698,     1] loss: 0.00130850\n",
      "[699,     1] loss: 0.00130003\n",
      "[700,     1] loss: 0.00129147\n",
      "[701,     1] loss: 0.00128312\n",
      "[702,     1] loss: 0.00127476\n",
      "[703,     1] loss: 0.00126650\n",
      "[704,     1] loss: 0.00125828\n",
      "[705,     1] loss: 0.00125010\n",
      "[706,     1] loss: 0.00124200\n",
      "[707,     1] loss: 0.00123390\n",
      "[708,     1] loss: 0.00122587\n",
      "[709,     1] loss: 0.00121791\n",
      "[710,     1] loss: 0.00120997\n",
      "[711,     1] loss: 0.00120212\n",
      "[712,     1] loss: 0.00119438\n",
      "[713,     1] loss: 0.00118655\n",
      "[714,     1] loss: 0.00117897\n",
      "[715,     1] loss: 0.00117124\n",
      "[716,     1] loss: 0.00116372\n",
      "[717,     1] loss: 0.00115614\n",
      "[718,     1] loss: 0.00114869\n",
      "[719,     1] loss: 0.00114131\n",
      "[720,     1] loss: 0.00113379\n",
      "[721,     1] loss: 0.00112648\n",
      "[722,     1] loss: 0.00111923\n",
      "[723,     1] loss: 0.00111200\n",
      "[724,     1] loss: 0.00110482\n",
      "[725,     1] loss: 0.00109773\n",
      "[726,     1] loss: 0.00109060\n",
      "[727,     1] loss: 0.00108357\n",
      "[728,     1] loss: 0.00107656\n",
      "[729,     1] loss: 0.00106967\n",
      "[730,     1] loss: 0.00106278\n",
      "[731,     1] loss: 0.00105587\n",
      "[732,     1] loss: 0.00104901\n",
      "[733,     1] loss: 0.00104233\n",
      "[734,     1] loss: 0.00103556\n",
      "[735,     1] loss: 0.00102894\n",
      "[736,     1] loss: 0.00102229\n",
      "[737,     1] loss: 0.00101573\n",
      "[738,     1] loss: 0.00100922\n",
      "[739,     1] loss: 0.00100265\n",
      "[740,     1] loss: 0.00099626\n",
      "[741,     1] loss: 0.00098987\n",
      "[742,     1] loss: 0.00098355\n",
      "[743,     1] loss: 0.00097720\n",
      "[744,     1] loss: 0.00097099\n",
      "[745,     1] loss: 0.00096472\n",
      "[746,     1] loss: 0.00095853\n",
      "[747,     1] loss: 0.00095238\n",
      "[748,     1] loss: 0.00094631\n",
      "[749,     1] loss: 0.00094021\n",
      "[750,     1] loss: 0.00093413\n",
      "[751,     1] loss: 0.00092812\n",
      "[752,     1] loss: 0.00092229\n",
      "[753,     1] loss: 0.00091635\n",
      "[754,     1] loss: 0.00091045\n",
      "[755,     1] loss: 0.00090468\n",
      "[756,     1] loss: 0.00089887\n",
      "[757,     1] loss: 0.00089307\n",
      "[758,     1] loss: 0.00088742\n",
      "[759,     1] loss: 0.00088181\n",
      "[760,     1] loss: 0.00087611\n",
      "[761,     1] loss: 0.00087057\n",
      "[762,     1] loss: 0.00086489\n",
      "[763,     1] loss: 0.00085941\n",
      "[764,     1] loss: 0.00085398\n",
      "[765,     1] loss: 0.00084854\n",
      "[766,     1] loss: 0.00084317\n",
      "[767,     1] loss: 0.00083767\n",
      "[768,     1] loss: 0.00083236\n",
      "[769,     1] loss: 0.00082702\n",
      "[770,     1] loss: 0.00082178\n",
      "[771,     1] loss: 0.00081653\n",
      "[772,     1] loss: 0.00081134\n",
      "[773,     1] loss: 0.00080613\n",
      "[774,     1] loss: 0.00080111\n",
      "[775,     1] loss: 0.00079599\n",
      "[776,     1] loss: 0.00079091\n",
      "[777,     1] loss: 0.00078580\n",
      "[778,     1] loss: 0.00078079\n",
      "[779,     1] loss: 0.00077592\n",
      "[780,     1] loss: 0.00077094\n",
      "[781,     1] loss: 0.00076611\n",
      "[782,     1] loss: 0.00076120\n",
      "[783,     1] loss: 0.00075629\n",
      "[784,     1] loss: 0.00075157\n",
      "[785,     1] loss: 0.00074674\n",
      "[786,     1] loss: 0.00074206\n",
      "[787,     1] loss: 0.00073743\n",
      "[788,     1] loss: 0.00073270\n",
      "[789,     1] loss: 0.00072807\n",
      "[790,     1] loss: 0.00072336\n",
      "[791,     1] loss: 0.00071881\n",
      "[792,     1] loss: 0.00071427\n",
      "[793,     1] loss: 0.00070979\n",
      "[794,     1] loss: 0.00070530\n",
      "[795,     1] loss: 0.00070077\n",
      "[796,     1] loss: 0.00069637\n",
      "[797,     1] loss: 0.00069195\n",
      "[798,     1] loss: 0.00068759\n",
      "[799,     1] loss: 0.00068325\n",
      "[800,     1] loss: 0.00067895\n",
      "[801,     1] loss: 0.00067465\n",
      "[802,     1] loss: 0.00067039\n",
      "[803,     1] loss: 0.00066616\n",
      "[804,     1] loss: 0.00066192\n",
      "[805,     1] loss: 0.00065782\n",
      "[806,     1] loss: 0.00065367\n",
      "[807,     1] loss: 0.00064952\n",
      "[808,     1] loss: 0.00064539\n",
      "[809,     1] loss: 0.00064128\n",
      "[810,     1] loss: 0.00063735\n",
      "[811,     1] loss: 0.00063327\n",
      "[812,     1] loss: 0.00062923\n",
      "[813,     1] loss: 0.00062535\n",
      "[814,     1] loss: 0.00062133\n",
      "[815,     1] loss: 0.00061751\n",
      "[816,     1] loss: 0.00061358\n",
      "[817,     1] loss: 0.00060974\n",
      "[818,     1] loss: 0.00060585\n",
      "[819,     1] loss: 0.00060210\n",
      "[820,     1] loss: 0.00059822\n",
      "[821,     1] loss: 0.00059454\n",
      "[822,     1] loss: 0.00059084\n",
      "[823,     1] loss: 0.00058708\n",
      "[824,     1] loss: 0.00058343\n",
      "[825,     1] loss: 0.00057982\n",
      "[826,     1] loss: 0.00057604\n",
      "[827,     1] loss: 0.00057243\n",
      "[828,     1] loss: 0.00056888\n",
      "[829,     1] loss: 0.00056537\n",
      "[830,     1] loss: 0.00056170\n",
      "[831,     1] loss: 0.00055821\n",
      "[832,     1] loss: 0.00055473\n",
      "[833,     1] loss: 0.00055128\n",
      "[834,     1] loss: 0.00054782\n",
      "[835,     1] loss: 0.00054438\n",
      "[836,     1] loss: 0.00054097\n",
      "[837,     1] loss: 0.00053759\n",
      "[838,     1] loss: 0.00053421\n",
      "[839,     1] loss: 0.00053087\n",
      "[840,     1] loss: 0.00052750\n",
      "[841,     1] loss: 0.00052422\n",
      "[842,     1] loss: 0.00052092\n",
      "[843,     1] loss: 0.00051776\n",
      "[844,     1] loss: 0.00051449\n",
      "[845,     1] loss: 0.00051124\n",
      "[846,     1] loss: 0.00050803\n",
      "[847,     1] loss: 0.00050484\n",
      "[848,     1] loss: 0.00050174\n",
      "[849,     1] loss: 0.00049857\n",
      "[850,     1] loss: 0.00049542\n",
      "[851,     1] loss: 0.00049240\n",
      "[852,     1] loss: 0.00048927\n",
      "[853,     1] loss: 0.00048617\n",
      "[854,     1] loss: 0.00048324\n",
      "[855,     1] loss: 0.00048019\n",
      "[856,     1] loss: 0.00047726\n",
      "[857,     1] loss: 0.00047421\n",
      "[858,     1] loss: 0.00047132\n",
      "[859,     1] loss: 0.00046832\n",
      "[860,     1] loss: 0.00046548\n",
      "[861,     1] loss: 0.00046250\n",
      "[862,     1] loss: 0.00045968\n",
      "[863,     1] loss: 0.00045677\n",
      "[864,     1] loss: 0.00045396\n",
      "[865,     1] loss: 0.00045119\n",
      "[866,     1] loss: 0.00044830\n",
      "[867,     1] loss: 0.00044555\n",
      "[868,     1] loss: 0.00044283\n",
      "[869,     1] loss: 0.00043996\n",
      "[870,     1] loss: 0.00043723\n",
      "[871,     1] loss: 0.00043456\n",
      "[872,     1] loss: 0.00043192\n",
      "[873,     1] loss: 0.00042917\n",
      "[874,     1] loss: 0.00042649\n",
      "[875,     1] loss: 0.00042390\n",
      "[876,     1] loss: 0.00042127\n",
      "[877,     1] loss: 0.00041868\n",
      "[878,     1] loss: 0.00041611\n",
      "[879,     1] loss: 0.00041341\n",
      "[880,     1] loss: 0.00041084\n",
      "[881,     1] loss: 0.00040830\n",
      "[882,     1] loss: 0.00040577\n",
      "[883,     1] loss: 0.00040328\n",
      "[884,     1] loss: 0.00040078\n",
      "[885,     1] loss: 0.00039829\n",
      "[886,     1] loss: 0.00039582\n",
      "[887,     1] loss: 0.00039333\n",
      "[888,     1] loss: 0.00039089\n",
      "[889,     1] loss: 0.00038848\n",
      "[890,     1] loss: 0.00038604\n",
      "[891,     1] loss: 0.00038375\n",
      "[892,     1] loss: 0.00038135\n",
      "[893,     1] loss: 0.00037898\n",
      "[894,     1] loss: 0.00037659\n",
      "[895,     1] loss: 0.00037426\n",
      "[896,     1] loss: 0.00037191\n",
      "[897,     1] loss: 0.00036970\n",
      "[898,     1] loss: 0.00036741\n",
      "[899,     1] loss: 0.00036511\n",
      "[900,     1] loss: 0.00036279\n",
      "[901,     1] loss: 0.00036066\n",
      "[902,     1] loss: 0.00035841\n",
      "[903,     1] loss: 0.00035617\n",
      "[904,     1] loss: 0.00035403\n",
      "[905,     1] loss: 0.00035181\n",
      "[906,     1] loss: 0.00034962\n",
      "[907,     1] loss: 0.00034752\n",
      "[908,     1] loss: 0.00034531\n",
      "[909,     1] loss: 0.00034314\n",
      "[910,     1] loss: 0.00034109\n",
      "[911,     1] loss: 0.00033896\n",
      "[912,     1] loss: 0.00033694\n",
      "[913,     1] loss: 0.00033481\n",
      "[914,     1] loss: 0.00033280\n",
      "[915,     1] loss: 0.00033068\n",
      "[916,     1] loss: 0.00032869\n",
      "[917,     1] loss: 0.00032662\n",
      "[918,     1] loss: 0.00032466\n",
      "[919,     1] loss: 0.00032263\n",
      "[920,     1] loss: 0.00032067\n",
      "[921,     1] loss: 0.00031876\n",
      "[922,     1] loss: 0.00031674\n",
      "[923,     1] loss: 0.00031483\n",
      "[924,     1] loss: 0.00031281\n",
      "[925,     1] loss: 0.00031092\n",
      "[926,     1] loss: 0.00030904\n",
      "[927,     1] loss: 0.00030709\n",
      "[928,     1] loss: 0.00030522\n",
      "[929,     1] loss: 0.00030338\n",
      "[930,     1] loss: 0.00030145\n",
      "[931,     1] loss: 0.00029961\n",
      "[932,     1] loss: 0.00029780\n",
      "[933,     1] loss: 0.00029602\n",
      "[934,     1] loss: 0.00029411\n",
      "[935,     1] loss: 0.00029234\n",
      "[936,     1] loss: 0.00029059\n",
      "[937,     1] loss: 0.00028881\n",
      "[938,     1] loss: 0.00028707\n",
      "[939,     1] loss: 0.00028522\n",
      "[940,     1] loss: 0.00028352\n",
      "[941,     1] loss: 0.00028176\n",
      "[942,     1] loss: 0.00028005\n",
      "[943,     1] loss: 0.00027836\n",
      "[944,     1] loss: 0.00027665\n",
      "[945,     1] loss: 0.00027494\n",
      "[946,     1] loss: 0.00027319\n",
      "[947,     1] loss: 0.00027151\n",
      "[948,     1] loss: 0.00026987\n",
      "[949,     1] loss: 0.00026822\n",
      "[950,     1] loss: 0.00026657\n",
      "[951,     1] loss: 0.00026496\n",
      "[952,     1] loss: 0.00026333\n",
      "[953,     1] loss: 0.00026174\n",
      "[954,     1] loss: 0.00026013\n",
      "[955,     1] loss: 0.00025851\n",
      "[956,     1] loss: 0.00025695\n",
      "[957,     1] loss: 0.00025536\n",
      "[958,     1] loss: 0.00025381\n",
      "[959,     1] loss: 0.00025226\n",
      "[960,     1] loss: 0.00025080\n",
      "[961,     1] loss: 0.00024927\n",
      "[962,     1] loss: 0.00024773\n",
      "[963,     1] loss: 0.00024620\n",
      "[964,     1] loss: 0.00024469\n",
      "[965,     1] loss: 0.00024318\n",
      "[966,     1] loss: 0.00024164\n",
      "[967,     1] loss: 0.00024016\n",
      "[968,     1] loss: 0.00023880\n",
      "[969,     1] loss: 0.00023734\n",
      "[970,     1] loss: 0.00023587\n",
      "[971,     1] loss: 0.00023440\n",
      "[972,     1] loss: 0.00023295\n",
      "[973,     1] loss: 0.00023163\n",
      "[974,     1] loss: 0.00023020\n",
      "[975,     1] loss: 0.00022875\n",
      "[976,     1] loss: 0.00022733\n",
      "[977,     1] loss: 0.00022605\n",
      "[978,     1] loss: 0.00022462\n",
      "[979,     1] loss: 0.00022322\n",
      "[980,     1] loss: 0.00022182\n",
      "[981,     1] loss: 0.00022054\n",
      "[982,     1] loss: 0.00021920\n",
      "[983,     1] loss: 0.00021782\n",
      "[984,     1] loss: 0.00021657\n",
      "[985,     1] loss: 0.00021521\n",
      "[986,     1] loss: 0.00021386\n",
      "[987,     1] loss: 0.00021265\n",
      "[988,     1] loss: 0.00021130\n",
      "[989,     1] loss: 0.00020999\n",
      "[990,     1] loss: 0.00020876\n",
      "[991,     1] loss: 0.00020745\n",
      "[992,     1] loss: 0.00020624\n",
      "[993,     1] loss: 0.00020496\n",
      "[994,     1] loss: 0.00020365\n",
      "[995,     1] loss: 0.00020247\n",
      "[996,     1] loss: 0.00020120\n",
      "[997,     1] loss: 0.00020005\n",
      "[998,     1] loss: 0.00019875\n",
      "[999,     1] loss: 0.00019763\n",
      "[1000,     1] loss: 0.00019635\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "batch_size = 50\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(list(reward_net.parameters()), lr=0.001)\n",
    "\n",
    "losses = []\n",
    "\n",
    "idxs = np.array(range(len(states1)))\n",
    "num_batches = len(idxs) // batch_size\n",
    "\n",
    "# Train the model with regular SGD\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "   \n",
    "        t_states1 = torch.Tensor(states1).float().to(device)\n",
    "        t_states2 = torch.Tensor(states2).float().to(device)\n",
    "        t_prefs = torch.Tensor(prefs).float().to(device).unsqueeze(1)\n",
    "        t_features1a = torch.Tensor(features1a).float().to(device)\n",
    "        t_features1b = torch.Tensor(features1b).float().to(device)\n",
    "        t_prefs_feature1 = torch.Tensor(prefs_feature1).float().to(device).unsqueeze(1)\n",
    "        t_features2a = torch.Tensor(features2a).float().to(device)\n",
    "        t_features2b = torch.Tensor(features2b).float().to(device)\n",
    "        t_prefs_feature2 = torch.Tensor(prefs_feature2).float().to(device).unsqueeze(1)\n",
    "        t_features3a = torch.Tensor(features3a).float().to(device)\n",
    "        t_features3b = torch.Tensor(features3b).float().to(device)\n",
    "        t_prefs_feature3 = torch.Tensor(prefs_feature3).float().to(device).unsqueeze(1)\n",
    "        t_features4a = torch.Tensor(features4a).float().to(device)\n",
    "        t_features4b = torch.Tensor(features4b).float().to(device)\n",
    "        t_prefs_feature4 = torch.Tensor(prefs_feature4).float().to(device).unsqueeze(1)\n",
    "        t_features5a = torch.Tensor(features5a).float().to(device)\n",
    "        t_features5b = torch.Tensor(features5b).float().to(device)\n",
    "        t_prefs_feature5 = torch.Tensor(prefs_feature5).float().to(device).unsqueeze(1)\n",
    "        t_features6a = torch.Tensor(features6a).float().to(device)\n",
    "        t_features6b = torch.Tensor(features6b).float().to(device)\n",
    "        t_prefs_feature6 = torch.Tensor(prefs_feature6).float().to(device).unsqueeze(1)\n",
    "        pred_prefs_feature1, pred_prefs_feature2, pred_prefs_feature3, pred_prefs_feature4, pred_prefs_feature5, pred_prefs_feature6, pred_prefs = reward_net(t_features1a, t_features1b, t_features2a, t_features2b, t_features3a, t_features3b, \n",
    "                                                                                                                           t_features4a, t_features4b, t_features5a, t_features5b, t_features6a, t_features6b, \n",
    "                                                                                                                           t_states1, t_states2)\n",
    "        feature1_loss = criterion(pred_prefs_feature1, t_prefs_feature1)\n",
    "        feature2_loss = criterion(pred_prefs_feature2, t_prefs_feature2)\n",
    "        feature3_loss = criterion(pred_prefs_feature3, t_prefs_feature3)\n",
    "        feature4_loss = criterion(pred_prefs_feature4, t_prefs_feature4)\n",
    "        feature5_loss = criterion(pred_prefs_feature5, t_prefs_feature5)\n",
    "        feature6_loss = criterion(pred_prefs_feature6, t_prefs_feature6)\n",
    "        state_loss = criterion(pred_prefs, t_prefs)\n",
    "        loss = feature1_loss + feature2_loss + feature3_loss + feature4_loss + feature5_loss + feature6_loss + state_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        losses.append(loss.item())\n",
    "\n",
    "torch.save(reward_net, 'reward_network.pt')\n",
    "print('Finished Training')\n",
    "plt.plot(losses)\n",
    "plt.savefig('losses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8d58003-2c9e-43d4-a766-356a8e37ebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct: 1.000000 \n"
     ]
    }
   ],
   "source": [
    "reward_net = torch.load('reward_network.pt')\n",
    "reward_net.eval()\n",
    "\n",
    "import csv\n",
    "with open('data/test_rewards.csv') as file_obj:\n",
    "    reader_obj = csv.reader(file_obj)\n",
    "\n",
    "    states1, states2, prefs = [], [], []\n",
    "    features1a, features1b, prefs_feature1 = [], [], []\n",
    "    features2a, features2b, prefs_feature2 = [], [], []\n",
    "    features3a, features3b, prefs_feature3 = [], [], []\n",
    "    features4a, features4b, prefs_feature4 = [], [], []\n",
    "    features5a, features5b, prefs_feature5 = [], [], []\n",
    "    features6a, features6b, prefs_feature6 = [], [], []\n",
    "    for row in reader_obj:\n",
    "        states1.append(row[0:18])\n",
    "        states2.append(row[19:37])\n",
    "        prefs.append(row[38])\n",
    "        features1a.append(row[0:3])\n",
    "        features1b.append(row[19:22])\n",
    "        prefs_feature1.append(row[39])\n",
    "        features2a.append(row[3:6])\n",
    "        features2b.append(row[22:25])\n",
    "        prefs_feature2.append(row[40])\n",
    "        features3a.append(row[6:9])\n",
    "        features3b.append(row[25:28])\n",
    "        prefs_feature3.append(row[41])\n",
    "        features4a.append(row[9:12])\n",
    "        features4b.append(row[28:31])\n",
    "        prefs_feature4.append(row[42])\n",
    "        features5a.append(row[12:15])\n",
    "        features5b.append(row[31:34])\n",
    "        prefs_feature5.append(row[43])\n",
    "        features6a.append(row[15:18])\n",
    "        features6b.append(row[34:37])\n",
    "        prefs_feature6.append(row[44])\n",
    "    states1 = np.array(states1,dtype=int)\n",
    "    states2 = np.array(states2,dtype=int)\n",
    "    prefs = np.array(prefs,dtype=int)\n",
    "    features1a = np.array(features1a,dtype=int)\n",
    "    features1b = np.array(features1b,dtype=int)\n",
    "    prefs_feature1 = np.array(prefs_feature1,dtype=int)\n",
    "    features2a = np.array(features2a,dtype=int)\n",
    "    features2b = np.array(features2b,dtype=int)\n",
    "    prefs_feature2 = np.array(prefs_feature2,dtype=int)\n",
    "    features3a = np.array(features3a,dtype=int)\n",
    "    features3b = np.array(features3b,dtype=int)\n",
    "    prefs_feature3 = np.array(prefs_feature3,dtype=int)\n",
    "    features4a = np.array(features4a,dtype=int)\n",
    "    features4b = np.array(features4b,dtype=int)\n",
    "    prefs_feature4 = np.array(prefs_feature4,dtype=int)\n",
    "    features5a = np.array(features5a,dtype=int)\n",
    "    features5b = np.array(features5b,dtype=int)\n",
    "    prefs_feature5 = np.array(prefs_feature5,dtype=int)\n",
    "    features6a = np.array(features6a,dtype=int)\n",
    "    features6b = np.array(features6b,dtype=int)\n",
    "    prefs_feature6 = np.array(prefs_feature6,dtype=int)\n",
    "\n",
    "num_correct = 0.0\n",
    "for i in range(len(states1)):\n",
    "    state1 = torch.Tensor(states1[i]).to(device)\n",
    "    state2 = torch.Tensor(states2[i]).to(device)\n",
    "    feature1a = torch.Tensor(features1a[i]).to(device)\n",
    "    feature1b = torch.Tensor(features1b[i]).to(device)\n",
    "    feature2a = torch.Tensor(features2a[i]).to(device)\n",
    "    feature2b = torch.Tensor(features2b[i]).to(device)\n",
    "    feature3a = torch.Tensor(features3a[i]).to(device)\n",
    "    feature3b = torch.Tensor(features3b[i]).to(device)\n",
    "    feature4a = torch.Tensor(features4a[i]).to(device)\n",
    "    feature4b = torch.Tensor(features4b[i]).to(device)\n",
    "    feature5a = torch.Tensor(features5a[i]).to(device)\n",
    "    feature5b = torch.Tensor(features5b[i]).to(device)\n",
    "    feature6a = torch.Tensor(features6a[i]).to(device)\n",
    "    feature6b = torch.Tensor(features6b[i]).to(device)\n",
    "    pref_feature1, pref_feature2, pref_feature3, pref_feature4, pref_feature5, pref_feature6, pred_pref = reward_net(feature1a, feature1b, feature2a, feature2b, feature3a, feature3b, \n",
    "                                                                                                                feature4a, feature4b, feature5a, feature5b, feature6a, feature6b, \n",
    "                                                                                                                state1, state2)\n",
    "    pred_pref = torch.sigmoid(pred_pref).cpu().detach().numpy()[0]\n",
    "    if pred_pref > 0.5 and prefs[i] == 1:\n",
    "        num_correct+=1\n",
    "    elif pred_pref <= 0.5 and prefs[i] == 0:\n",
    "        num_correct+=1\n",
    "\n",
    "accuracy = num_correct / len(states1)\n",
    "print(\"Percent correct: %f \" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06a999-fb81-4a24-86d9-a046bdbccee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-preference",
   "language": "python",
   "name": "feature-preference"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
