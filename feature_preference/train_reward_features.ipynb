{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3247ef59-99f4-47a9-99ba-61ab9d63e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions as pyd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc2b5ffe-ffe0-4ebf-8a35-1f805babc574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "class RewardNetFeature(nn.Module):\n",
    "    def __init__(\n",
    "        self, feature_dim, output_mod=None):\n",
    "        super().__init__()\n",
    "        self.feature1 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.feature2 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.feature3 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.feature4 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.feature5 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.feature6 = nn.Sequential(\n",
    "            nn.Linear(feature_dim, 32), nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        self.pref_feature1 = nn.Linear(2,1)\n",
    "        self.pref_feature2 = nn.Linear(2,1)\n",
    "        self.pref_feature3 = nn.Linear(2,1)\n",
    "        self.pref_feature4 = nn.Linear(2,1)\n",
    "        self.pref_feature5 = nn.Linear(2,1)\n",
    "        self.pref_feature6 = nn.Linear(2,1)\n",
    "        self.pref = nn.Linear(6, 1)\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, feature1a, feature1b, feature2a, feature2b, feature3a, feature3b, \n",
    "                feature4a, feature4b, feature5a, feature5b, feature6a, feature6b, state1, state2):\n",
    "        feature1a = self.feature1(feature1a)\n",
    "        feature1b = self.feature1(feature1b)\n",
    "        feature2a = self.feature2(feature2a)\n",
    "        feature2b = self.feature2(feature2b)\n",
    "        feature3a = self.feature3(feature3a)\n",
    "        feature3b = self.feature3(feature3b)\n",
    "        feature4a = self.feature4(feature4a)\n",
    "        feature4b = self.feature4(feature4b)\n",
    "        feature5a = self.feature5(feature5a)\n",
    "        feature5b = self.feature5(feature5b)\n",
    "        feature6a = self.feature6(feature6a)\n",
    "        feature6b = self.feature6(feature6b)\n",
    "        pref_feature1 = self.pref_feature1(torch.squeeze(torch.stack([feature1a,feature1b], dim=1)))\n",
    "        pref_feature2 = self.pref_feature2(torch.squeeze(torch.stack([feature2a,feature2b], dim=1)))\n",
    "        pref_feature3 = self.pref_feature3(torch.squeeze(torch.stack([feature3a,feature3b], dim=1)))\n",
    "        pref_feature4 = self.pref_feature4(torch.squeeze(torch.stack([feature4a,feature4b], dim=1)))\n",
    "        pref_feature5 = self.pref_feature5(torch.squeeze(torch.stack([feature5a,feature5b], dim=1)))\n",
    "        pref_feature6 = self.pref_feature6(torch.squeeze(torch.stack([feature6a,feature6b], dim=1)))\n",
    "        pref = self.pref(torch.squeeze(torch.stack([pref_feature1, pref_feature2, pref_feature3, \n",
    "                                                    pref_feature4, pref_feature5, pref_feature6], dim=1)))\n",
    "        return pref_feature1, pref_feature2, pref_feature3, pref_feature4, pref_feature5, pref_feature6, pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c23618a1-3246-4c6d-877a-8209977f4488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardNetFeature(\n",
       "  (feature1): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (feature2): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (feature3): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (feature4): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (feature5): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (feature6): Sequential(\n",
       "    (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (pref_feature1): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref_feature2): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref_feature3): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref_feature4): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref_feature5): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref_feature6): Linear(in_features=2, out_features=1, bias=True)\n",
       "  (pref): Linear(in_features=6, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dim = 3\n",
    "\n",
    "reward_net = RewardNetFeature(feature_dim)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "reward_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab01bacf-7a31-462a-b2d9-c857491ba2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data/train_rewards10.csv') as file_obj:\n",
    "    reader_obj = csv.reader(file_obj)\n",
    "\n",
    "    states1, states2, prefs = [], [], []\n",
    "    features1a, features1b, prefs_feature1 = [], [], []\n",
    "    features2a, features2b, prefs_feature2 = [], [], []\n",
    "    features3a, features3b, prefs_feature3 = [], [], []\n",
    "    features4a, features4b, prefs_feature4 = [], [], []\n",
    "    features5a, features5b, prefs_feature5 = [], [], []\n",
    "    features6a, features6b, prefs_feature6 = [], [], []\n",
    "    for row in reader_obj:\n",
    "        states1.append(row[0:18])\n",
    "        states2.append(row[19:37])\n",
    "        prefs.append(row[38])\n",
    "        features1a.append(row[0:3])\n",
    "        features1b.append(row[19:22])\n",
    "        prefs_feature1.append(row[39])\n",
    "        features2a.append(row[3:6])\n",
    "        features2b.append(row[22:25])\n",
    "        prefs_feature2.append(row[40])\n",
    "        features3a.append(row[6:9])\n",
    "        features3b.append(row[25:28])\n",
    "        prefs_feature3.append(row[41])\n",
    "        features4a.append(row[9:12])\n",
    "        features4b.append(row[28:31])\n",
    "        prefs_feature4.append(row[42])\n",
    "        features5a.append(row[12:15])\n",
    "        features5b.append(row[31:34])\n",
    "        prefs_feature5.append(row[43])\n",
    "        features6a.append(row[15:18])\n",
    "        features6b.append(row[34:37])\n",
    "        prefs_feature6.append(row[44])\n",
    "    states1 = np.array(states1,dtype=int)\n",
    "    states2 = np.array(states2,dtype=int)\n",
    "    prefs = np.array(prefs,dtype=int)\n",
    "    features1a = np.array(features1a,dtype=int)\n",
    "    features1b = np.array(features1b,dtype=int)\n",
    "    prefs_feature1 = np.array(prefs_feature1,dtype=int)\n",
    "    features2a = np.array(features2a,dtype=int)\n",
    "    features2b = np.array(features2b,dtype=int)\n",
    "    prefs_feature2 = np.array(prefs_feature2,dtype=int)\n",
    "    features3a = np.array(features3a,dtype=int)\n",
    "    features3b = np.array(features3b,dtype=int)\n",
    "    prefs_feature3 = np.array(prefs_feature3,dtype=int)\n",
    "    features4a = np.array(features4a,dtype=int)\n",
    "    features4b = np.array(features4b,dtype=int)\n",
    "    prefs_feature4 = np.array(prefs_feature4,dtype=int)\n",
    "    features5a = np.array(features5a,dtype=int)\n",
    "    features5b = np.array(features5b,dtype=int)\n",
    "    prefs_feature5 = np.array(prefs_feature5,dtype=int)\n",
    "    features6a = np.array(features6a,dtype=int)\n",
    "    features6b = np.array(features6b,dtype=int)\n",
    "    prefs_feature6 = np.array(prefs_feature6,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea49771b-bbf6-4f0e-b0d7-4c7acdea7211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 4.77241802\n",
      "[2,     1] loss: 4.75228357\n",
      "[3,     1] loss: 4.73231936\n",
      "[4,     1] loss: 4.71244621\n",
      "[5,     1] loss: 4.69266176\n",
      "[6,     1] loss: 4.67298365\n",
      "[7,     1] loss: 4.65344524\n",
      "[8,     1] loss: 4.63398361\n",
      "[9,     1] loss: 4.61454630\n",
      "[10,     1] loss: 4.59521770\n",
      "[11,     1] loss: 4.57599211\n",
      "[12,     1] loss: 4.55684519\n",
      "[13,     1] loss: 4.53776884\n",
      "[14,     1] loss: 4.51880026\n",
      "[15,     1] loss: 4.49995661\n",
      "[16,     1] loss: 4.48123312\n",
      "[17,     1] loss: 4.46262836\n",
      "[18,     1] loss: 4.44414663\n",
      "[19,     1] loss: 4.42571688\n",
      "[20,     1] loss: 4.40727901\n",
      "[21,     1] loss: 4.38894606\n",
      "[22,     1] loss: 4.37074709\n",
      "[23,     1] loss: 4.35263062\n",
      "[24,     1] loss: 4.33457470\n",
      "[25,     1] loss: 4.31657076\n",
      "[26,     1] loss: 4.29856777\n",
      "[27,     1] loss: 4.28058672\n",
      "[28,     1] loss: 4.26264334\n",
      "[29,     1] loss: 4.24471855\n",
      "[30,     1] loss: 4.22681475\n",
      "[31,     1] loss: 4.20893717\n",
      "[32,     1] loss: 4.19110680\n",
      "[33,     1] loss: 4.17330313\n",
      "[34,     1] loss: 4.15559101\n",
      "[35,     1] loss: 4.13790083\n",
      "[36,     1] loss: 4.12026882\n",
      "[37,     1] loss: 4.10266685\n",
      "[38,     1] loss: 4.08509350\n",
      "[39,     1] loss: 4.06754351\n",
      "[40,     1] loss: 4.04998922\n",
      "[41,     1] loss: 4.03240108\n",
      "[42,     1] loss: 4.01480675\n",
      "[43,     1] loss: 3.99720407\n",
      "[44,     1] loss: 3.97960520\n",
      "[45,     1] loss: 3.96203399\n",
      "[46,     1] loss: 3.94447184\n",
      "[47,     1] loss: 3.92691541\n",
      "[48,     1] loss: 3.90932345\n",
      "[49,     1] loss: 3.89170098\n",
      "[50,     1] loss: 3.87406492\n",
      "[51,     1] loss: 3.85642099\n",
      "[52,     1] loss: 3.83878899\n",
      "[53,     1] loss: 3.82113481\n",
      "[54,     1] loss: 3.80344152\n",
      "[55,     1] loss: 3.78572226\n",
      "[56,     1] loss: 3.76802206\n",
      "[57,     1] loss: 3.75032687\n",
      "[58,     1] loss: 3.73264074\n",
      "[59,     1] loss: 3.71495008\n",
      "[60,     1] loss: 3.69726944\n",
      "[61,     1] loss: 3.67960548\n",
      "[62,     1] loss: 3.66195345\n",
      "[63,     1] loss: 3.64431500\n",
      "[64,     1] loss: 3.62669611\n",
      "[65,     1] loss: 3.60910940\n",
      "[66,     1] loss: 3.59154677\n",
      "[67,     1] loss: 3.57401752\n",
      "[68,     1] loss: 3.55652523\n",
      "[69,     1] loss: 3.53906250\n",
      "[70,     1] loss: 3.52163148\n",
      "[71,     1] loss: 3.50419164\n",
      "[72,     1] loss: 3.48676252\n",
      "[73,     1] loss: 3.46935463\n",
      "[74,     1] loss: 3.45191813\n",
      "[75,     1] loss: 3.43445158\n",
      "[76,     1] loss: 3.41703701\n",
      "[77,     1] loss: 3.39964604\n",
      "[78,     1] loss: 3.38228536\n",
      "[79,     1] loss: 3.36499596\n",
      "[80,     1] loss: 3.34778881\n",
      "[81,     1] loss: 3.33064461\n",
      "[82,     1] loss: 3.31358600\n",
      "[83,     1] loss: 3.29653215\n",
      "[84,     1] loss: 3.27953029\n",
      "[85,     1] loss: 3.26260185\n",
      "[86,     1] loss: 3.24573851\n",
      "[87,     1] loss: 3.22891283\n",
      "[88,     1] loss: 3.21213579\n",
      "[89,     1] loss: 3.19542885\n",
      "[90,     1] loss: 3.17876911\n",
      "[91,     1] loss: 3.16214657\n",
      "[92,     1] loss: 3.14562035\n",
      "[93,     1] loss: 3.12919807\n",
      "[94,     1] loss: 3.11286783\n",
      "[95,     1] loss: 3.09661722\n",
      "[96,     1] loss: 3.08048177\n",
      "[97,     1] loss: 3.06439495\n",
      "[98,     1] loss: 3.04841900\n",
      "[99,     1] loss: 3.03255439\n",
      "[100,     1] loss: 3.01679516\n",
      "[101,     1] loss: 3.00114417\n",
      "[102,     1] loss: 2.98563147\n",
      "[103,     1] loss: 2.97023439\n",
      "[104,     1] loss: 2.95495224\n",
      "[105,     1] loss: 2.93977070\n",
      "[106,     1] loss: 2.92471409\n",
      "[107,     1] loss: 2.90969849\n",
      "[108,     1] loss: 2.89476585\n",
      "[109,     1] loss: 2.87996984\n",
      "[110,     1] loss: 2.86529112\n",
      "[111,     1] loss: 2.85071659\n",
      "[112,     1] loss: 2.83627462\n",
      "[113,     1] loss: 2.82196522\n",
      "[114,     1] loss: 2.80777359\n",
      "[115,     1] loss: 2.79368424\n",
      "[116,     1] loss: 2.77970123\n",
      "[117,     1] loss: 2.76585317\n",
      "[118,     1] loss: 2.75212336\n",
      "[119,     1] loss: 2.73852301\n",
      "[120,     1] loss: 2.72504807\n",
      "[121,     1] loss: 2.71172619\n",
      "[122,     1] loss: 2.69853687\n",
      "[123,     1] loss: 2.68545437\n",
      "[124,     1] loss: 2.67248774\n",
      "[125,     1] loss: 2.65967774\n",
      "[126,     1] loss: 2.64699459\n",
      "[127,     1] loss: 2.63438702\n",
      "[128,     1] loss: 2.62190008\n",
      "[129,     1] loss: 2.60952473\n",
      "[130,     1] loss: 2.59726310\n",
      "[131,     1] loss: 2.58517170\n",
      "[132,     1] loss: 2.57313967\n",
      "[133,     1] loss: 2.56124115\n",
      "[134,     1] loss: 2.54949355\n",
      "[135,     1] loss: 2.53782892\n",
      "[136,     1] loss: 2.52627015\n",
      "[137,     1] loss: 2.51481247\n",
      "[138,     1] loss: 2.50345325\n",
      "[139,     1] loss: 2.49221420\n",
      "[140,     1] loss: 2.48106742\n",
      "[141,     1] loss: 2.47003722\n",
      "[142,     1] loss: 2.45911551\n",
      "[143,     1] loss: 2.44827962\n",
      "[144,     1] loss: 2.43753290\n",
      "[145,     1] loss: 2.42685151\n",
      "[146,     1] loss: 2.41628361\n",
      "[147,     1] loss: 2.40583515\n",
      "[148,     1] loss: 2.39546251\n",
      "[149,     1] loss: 2.38513279\n",
      "[150,     1] loss: 2.37485933\n",
      "[151,     1] loss: 2.36467671\n",
      "[152,     1] loss: 2.35456657\n",
      "[153,     1] loss: 2.34453392\n",
      "[154,     1] loss: 2.33456540\n",
      "[155,     1] loss: 2.32464600\n",
      "[156,     1] loss: 2.31478977\n",
      "[157,     1] loss: 2.30501390\n",
      "[158,     1] loss: 2.29531240\n",
      "[159,     1] loss: 2.28565240\n",
      "[160,     1] loss: 2.27608418\n",
      "[161,     1] loss: 2.26658630\n",
      "[162,     1] loss: 2.25714350\n",
      "[163,     1] loss: 2.24774742\n",
      "[164,     1] loss: 2.23838377\n",
      "[165,     1] loss: 2.22904491\n",
      "[166,     1] loss: 2.21976137\n",
      "[167,     1] loss: 2.21052217\n",
      "[168,     1] loss: 2.20132279\n",
      "[169,     1] loss: 2.19214058\n",
      "[170,     1] loss: 2.18301439\n",
      "[171,     1] loss: 2.17394447\n",
      "[172,     1] loss: 2.16484690\n",
      "[173,     1] loss: 2.15574980\n",
      "[174,     1] loss: 2.14671469\n",
      "[175,     1] loss: 2.13768053\n",
      "[176,     1] loss: 2.12867832\n",
      "[177,     1] loss: 2.11971235\n",
      "[178,     1] loss: 2.11074996\n",
      "[179,     1] loss: 2.10184360\n",
      "[180,     1] loss: 2.09302211\n",
      "[181,     1] loss: 2.08427191\n",
      "[182,     1] loss: 2.07554221\n",
      "[183,     1] loss: 2.06687975\n",
      "[184,     1] loss: 2.05825806\n",
      "[185,     1] loss: 2.04970860\n",
      "[186,     1] loss: 2.04121518\n",
      "[187,     1] loss: 2.03273964\n",
      "[188,     1] loss: 2.02432942\n",
      "[189,     1] loss: 2.01596522\n",
      "[190,     1] loss: 2.00764132\n",
      "[191,     1] loss: 1.99936712\n",
      "[192,     1] loss: 1.99114239\n",
      "[193,     1] loss: 1.98295105\n",
      "[194,     1] loss: 1.97480845\n",
      "[195,     1] loss: 1.96672678\n",
      "[196,     1] loss: 1.95867801\n",
      "[197,     1] loss: 1.95067120\n",
      "[198,     1] loss: 1.94269681\n",
      "[199,     1] loss: 1.93481541\n",
      "[200,     1] loss: 1.92695796\n",
      "[201,     1] loss: 1.91909647\n",
      "[202,     1] loss: 1.91131091\n",
      "[203,     1] loss: 1.90358353\n",
      "[204,     1] loss: 1.89587677\n",
      "[205,     1] loss: 1.88818860\n",
      "[206,     1] loss: 1.88055634\n",
      "[207,     1] loss: 1.87299454\n",
      "[208,     1] loss: 1.86545384\n",
      "[209,     1] loss: 1.85793185\n",
      "[210,     1] loss: 1.85044730\n",
      "[211,     1] loss: 1.84301353\n",
      "[212,     1] loss: 1.83562791\n",
      "[213,     1] loss: 1.82826149\n",
      "[214,     1] loss: 1.82094216\n",
      "[215,     1] loss: 1.81365323\n",
      "[216,     1] loss: 1.80639803\n",
      "[217,     1] loss: 1.79920268\n",
      "[218,     1] loss: 1.79202080\n",
      "[219,     1] loss: 1.78485906\n",
      "[220,     1] loss: 1.77775788\n",
      "[221,     1] loss: 1.77069378\n",
      "[222,     1] loss: 1.76365900\n",
      "[223,     1] loss: 1.75665689\n",
      "[224,     1] loss: 1.74969411\n",
      "[225,     1] loss: 1.74275708\n",
      "[226,     1] loss: 1.73585176\n",
      "[227,     1] loss: 1.72900617\n",
      "[228,     1] loss: 1.72217429\n",
      "[229,     1] loss: 1.71536493\n",
      "[230,     1] loss: 1.70860326\n",
      "[231,     1] loss: 1.70188808\n",
      "[232,     1] loss: 1.69519532\n",
      "[233,     1] loss: 1.68852186\n",
      "[234,     1] loss: 1.68188941\n",
      "[235,     1] loss: 1.67527938\n",
      "[236,     1] loss: 1.66871190\n",
      "[237,     1] loss: 1.66216850\n",
      "[238,     1] loss: 1.65564322\n",
      "[239,     1] loss: 1.64914477\n",
      "[240,     1] loss: 1.64269876\n",
      "[241,     1] loss: 1.63630629\n",
      "[242,     1] loss: 1.62991142\n",
      "[243,     1] loss: 1.62354803\n",
      "[244,     1] loss: 1.61723900\n",
      "[245,     1] loss: 1.61094844\n",
      "[246,     1] loss: 1.60469437\n",
      "[247,     1] loss: 1.59846866\n",
      "[248,     1] loss: 1.59226990\n",
      "[249,     1] loss: 1.58611631\n",
      "[250,     1] loss: 1.57998192\n",
      "[251,     1] loss: 1.57388222\n",
      "[252,     1] loss: 1.56781673\n",
      "[253,     1] loss: 1.56178141\n",
      "[254,     1] loss: 1.55577362\n",
      "[255,     1] loss: 1.54978406\n",
      "[256,     1] loss: 1.54383874\n",
      "[257,     1] loss: 1.53794384\n",
      "[258,     1] loss: 1.53206122\n",
      "[259,     1] loss: 1.52620864\n",
      "[260,     1] loss: 1.52038431\n",
      "[261,     1] loss: 1.51460433\n",
      "[262,     1] loss: 1.50884795\n",
      "[263,     1] loss: 1.50310993\n",
      "[264,     1] loss: 1.49741149\n",
      "[265,     1] loss: 1.49172473\n",
      "[266,     1] loss: 1.48607659\n",
      "[267,     1] loss: 1.48046398\n",
      "[268,     1] loss: 1.47486699\n",
      "[269,     1] loss: 1.46926188\n",
      "[270,     1] loss: 1.46370077\n",
      "[271,     1] loss: 1.45815873\n",
      "[272,     1] loss: 1.45263958\n",
      "[273,     1] loss: 1.44714081\n",
      "[274,     1] loss: 1.44166636\n",
      "[275,     1] loss: 1.43623006\n",
      "[276,     1] loss: 1.43081737\n",
      "[277,     1] loss: 1.42538273\n",
      "[278,     1] loss: 1.41997063\n",
      "[279,     1] loss: 1.41457903\n",
      "[280,     1] loss: 1.40921545\n",
      "[281,     1] loss: 1.40388155\n",
      "[282,     1] loss: 1.39853573\n",
      "[283,     1] loss: 1.39320302\n",
      "[284,     1] loss: 1.38789392\n",
      "[285,     1] loss: 1.38260818\n",
      "[286,     1] loss: 1.37736022\n",
      "[287,     1] loss: 1.37213349\n",
      "[288,     1] loss: 1.36692619\n",
      "[289,     1] loss: 1.36173856\n",
      "[290,     1] loss: 1.35658240\n",
      "[291,     1] loss: 1.35145164\n",
      "[292,     1] loss: 1.34635711\n",
      "[293,     1] loss: 1.34128487\n",
      "[294,     1] loss: 1.33623874\n",
      "[295,     1] loss: 1.33122253\n",
      "[296,     1] loss: 1.32623339\n",
      "[297,     1] loss: 1.32128096\n",
      "[298,     1] loss: 1.31635511\n",
      "[299,     1] loss: 1.31144738\n",
      "[300,     1] loss: 1.30657077\n",
      "[301,     1] loss: 1.30172014\n",
      "[302,     1] loss: 1.29690289\n",
      "[303,     1] loss: 1.29210806\n",
      "[304,     1] loss: 1.28733850\n",
      "[305,     1] loss: 1.28260255\n",
      "[306,     1] loss: 1.27788055\n",
      "[307,     1] loss: 1.27319288\n",
      "[308,     1] loss: 1.26852608\n",
      "[309,     1] loss: 1.26388657\n",
      "[310,     1] loss: 1.25927615\n",
      "[311,     1] loss: 1.25468779\n",
      "[312,     1] loss: 1.25011647\n",
      "[313,     1] loss: 1.24556744\n",
      "[314,     1] loss: 1.24104071\n",
      "[315,     1] loss: 1.23654437\n",
      "[316,     1] loss: 1.23206878\n",
      "[317,     1] loss: 1.22761416\n",
      "[318,     1] loss: 1.22318041\n",
      "[319,     1] loss: 1.21877074\n",
      "[320,     1] loss: 1.21437597\n",
      "[321,     1] loss: 1.21000123\n",
      "[322,     1] loss: 1.20564365\n",
      "[323,     1] loss: 1.20131648\n",
      "[324,     1] loss: 1.19700658\n",
      "[325,     1] loss: 1.19271755\n",
      "[326,     1] loss: 1.18843746\n",
      "[327,     1] loss: 1.18417835\n",
      "[328,     1] loss: 1.17993879\n",
      "[329,     1] loss: 1.17571568\n",
      "[330,     1] loss: 1.17150855\n",
      "[331,     1] loss: 1.16732430\n",
      "[332,     1] loss: 1.16315663\n",
      "[333,     1] loss: 1.15899658\n",
      "[334,     1] loss: 1.15485263\n",
      "[335,     1] loss: 1.15072989\n",
      "[336,     1] loss: 1.14662290\n",
      "[337,     1] loss: 1.14253795\n",
      "[338,     1] loss: 1.13846946\n",
      "[339,     1] loss: 1.13441145\n",
      "[340,     1] loss: 1.13037157\n",
      "[341,     1] loss: 1.12634730\n",
      "[342,     1] loss: 1.12233770\n",
      "[343,     1] loss: 1.11834633\n",
      "[344,     1] loss: 1.11436796\n",
      "[345,     1] loss: 1.11040485\n",
      "[346,     1] loss: 1.10645723\n",
      "[347,     1] loss: 1.10252416\n",
      "[348,     1] loss: 1.09860563\n",
      "[349,     1] loss: 1.09469783\n",
      "[350,     1] loss: 1.09080410\n",
      "[351,     1] loss: 1.08692884\n",
      "[352,     1] loss: 1.08306742\n",
      "[353,     1] loss: 1.07922077\n",
      "[354,     1] loss: 1.07537889\n",
      "[355,     1] loss: 1.07155609\n",
      "[356,     1] loss: 1.06774402\n",
      "[357,     1] loss: 1.06394720\n",
      "[358,     1] loss: 1.06016088\n",
      "[359,     1] loss: 1.05638611\n",
      "[360,     1] loss: 1.05262828\n",
      "[361,     1] loss: 1.04887831\n",
      "[362,     1] loss: 1.04514205\n",
      "[363,     1] loss: 1.04141843\n",
      "[364,     1] loss: 1.03770685\n",
      "[365,     1] loss: 1.03400326\n",
      "[366,     1] loss: 1.03031397\n",
      "[367,     1] loss: 1.02663791\n",
      "[368,     1] loss: 1.02296984\n",
      "[369,     1] loss: 1.01930976\n",
      "[370,     1] loss: 1.01566267\n",
      "[371,     1] loss: 1.01202893\n",
      "[372,     1] loss: 1.00840497\n",
      "[373,     1] loss: 1.00479066\n",
      "[374,     1] loss: 1.00118613\n",
      "[375,     1] loss: 0.99758965\n",
      "[376,     1] loss: 0.99400449\n",
      "[377,     1] loss: 0.99042916\n",
      "[378,     1] loss: 0.98685980\n",
      "[379,     1] loss: 0.98330176\n",
      "[380,     1] loss: 0.97975397\n",
      "[381,     1] loss: 0.97621906\n",
      "[382,     1] loss: 0.97268993\n",
      "[383,     1] loss: 0.96916759\n",
      "[384,     1] loss: 0.96565497\n",
      "[385,     1] loss: 0.96215391\n",
      "[386,     1] loss: 0.95866400\n",
      "[387,     1] loss: 0.95518076\n",
      "[388,     1] loss: 0.95170313\n",
      "[389,     1] loss: 0.94823623\n",
      "[390,     1] loss: 0.94477761\n",
      "[391,     1] loss: 0.94132829\n",
      "[392,     1] loss: 0.93788660\n",
      "[393,     1] loss: 0.93445206\n",
      "[394,     1] loss: 0.93102676\n",
      "[395,     1] loss: 0.92760849\n",
      "[396,     1] loss: 0.92419606\n",
      "[397,     1] loss: 0.92078984\n",
      "[398,     1] loss: 0.91739798\n",
      "[399,     1] loss: 0.91401082\n",
      "[400,     1] loss: 0.91062760\n",
      "[401,     1] loss: 0.90725404\n",
      "[402,     1] loss: 0.90388751\n",
      "[403,     1] loss: 0.90052813\n",
      "[404,     1] loss: 0.89717561\n",
      "[405,     1] loss: 0.89382935\n",
      "[406,     1] loss: 0.89048946\n",
      "[407,     1] loss: 0.88715327\n",
      "[408,     1] loss: 0.88382620\n",
      "[409,     1] loss: 0.88050306\n",
      "[410,     1] loss: 0.87718660\n",
      "[411,     1] loss: 0.87387794\n",
      "[412,     1] loss: 0.87057316\n",
      "[413,     1] loss: 0.86727166\n",
      "[414,     1] loss: 0.86398125\n",
      "[415,     1] loss: 0.86069483\n",
      "[416,     1] loss: 0.85741615\n",
      "[417,     1] loss: 0.85413921\n",
      "[418,     1] loss: 0.85086238\n",
      "[419,     1] loss: 0.84758985\n",
      "[420,     1] loss: 0.84432250\n",
      "[421,     1] loss: 0.84105897\n",
      "[422,     1] loss: 0.83779860\n",
      "[423,     1] loss: 0.83454287\n",
      "[424,     1] loss: 0.83129442\n",
      "[425,     1] loss: 0.82805151\n",
      "[426,     1] loss: 0.82480884\n",
      "[427,     1] loss: 0.82157499\n",
      "[428,     1] loss: 0.81834275\n",
      "[429,     1] loss: 0.81511617\n",
      "[430,     1] loss: 0.81189436\n",
      "[431,     1] loss: 0.80867720\n",
      "[432,     1] loss: 0.80546308\n",
      "[433,     1] loss: 0.80225480\n",
      "[434,     1] loss: 0.79905212\n",
      "[435,     1] loss: 0.79585087\n",
      "[436,     1] loss: 0.79265618\n",
      "[437,     1] loss: 0.78946590\n",
      "[438,     1] loss: 0.78627884\n",
      "[439,     1] loss: 0.78309953\n",
      "[440,     1] loss: 0.77992344\n",
      "[441,     1] loss: 0.77674949\n",
      "[442,     1] loss: 0.77358335\n",
      "[443,     1] loss: 0.77041829\n",
      "[444,     1] loss: 0.76725948\n",
      "[445,     1] loss: 0.76410592\n",
      "[446,     1] loss: 0.76095676\n",
      "[447,     1] loss: 0.75781041\n",
      "[448,     1] loss: 0.75466967\n",
      "[449,     1] loss: 0.75153303\n",
      "[450,     1] loss: 0.74841195\n",
      "[451,     1] loss: 0.74527472\n",
      "[452,     1] loss: 0.74214995\n",
      "[453,     1] loss: 0.73903191\n",
      "[454,     1] loss: 0.73592067\n",
      "[455,     1] loss: 0.73281431\n",
      "[456,     1] loss: 0.72971153\n",
      "[457,     1] loss: 0.72661269\n",
      "[458,     1] loss: 0.72351873\n",
      "[459,     1] loss: 0.72043079\n",
      "[460,     1] loss: 0.71734929\n",
      "[461,     1] loss: 0.71427196\n",
      "[462,     1] loss: 0.71119928\n",
      "[463,     1] loss: 0.70813054\n",
      "[464,     1] loss: 0.70506835\n",
      "[465,     1] loss: 0.70201182\n",
      "[466,     1] loss: 0.69895893\n",
      "[467,     1] loss: 0.69591105\n",
      "[468,     1] loss: 0.69286984\n",
      "[469,     1] loss: 0.68982893\n",
      "[470,     1] loss: 0.68679976\n",
      "[471,     1] loss: 0.68377429\n",
      "[472,     1] loss: 0.68075228\n",
      "[473,     1] loss: 0.67773521\n",
      "[474,     1] loss: 0.67472529\n",
      "[475,     1] loss: 0.67172670\n",
      "[476,     1] loss: 0.66872513\n",
      "[477,     1] loss: 0.66573346\n",
      "[478,     1] loss: 0.66274738\n",
      "[479,     1] loss: 0.65976655\n",
      "[480,     1] loss: 0.65679246\n",
      "[481,     1] loss: 0.65382540\n",
      "[482,     1] loss: 0.65086114\n",
      "[483,     1] loss: 0.64790553\n",
      "[484,     1] loss: 0.64495963\n",
      "[485,     1] loss: 0.64201486\n",
      "[486,     1] loss: 0.63908005\n",
      "[487,     1] loss: 0.63615191\n",
      "[488,     1] loss: 0.63322794\n",
      "[489,     1] loss: 0.63031256\n",
      "[490,     1] loss: 0.62740344\n",
      "[491,     1] loss: 0.62450045\n",
      "[492,     1] loss: 0.62160766\n",
      "[493,     1] loss: 0.61871648\n",
      "[494,     1] loss: 0.61583734\n",
      "[495,     1] loss: 0.61296445\n",
      "[496,     1] loss: 0.61009943\n",
      "[497,     1] loss: 0.60724193\n",
      "[498,     1] loss: 0.60439169\n",
      "[499,     1] loss: 0.60154951\n",
      "[500,     1] loss: 0.59871459\n",
      "[501,     1] loss: 0.59588683\n",
      "[502,     1] loss: 0.59306645\n",
      "[503,     1] loss: 0.59025353\n",
      "[504,     1] loss: 0.58745164\n",
      "[505,     1] loss: 0.58465558\n",
      "[506,     1] loss: 0.58186746\n",
      "[507,     1] loss: 0.57908916\n",
      "[508,     1] loss: 0.57631713\n",
      "[509,     1] loss: 0.57355636\n",
      "[510,     1] loss: 0.57080197\n",
      "[511,     1] loss: 0.56806183\n",
      "[512,     1] loss: 0.56532466\n",
      "[513,     1] loss: 0.56259584\n",
      "[514,     1] loss: 0.55987918\n",
      "[515,     1] loss: 0.55717003\n",
      "[516,     1] loss: 0.55447012\n",
      "[517,     1] loss: 0.55178040\n",
      "[518,     1] loss: 0.54909873\n",
      "[519,     1] loss: 0.54642689\n",
      "[520,     1] loss: 0.54376584\n",
      "[521,     1] loss: 0.54111314\n",
      "[522,     1] loss: 0.53847021\n",
      "[523,     1] loss: 0.53583676\n",
      "[524,     1] loss: 0.53321475\n",
      "[525,     1] loss: 0.53059864\n",
      "[526,     1] loss: 0.52799469\n",
      "[527,     1] loss: 0.52540141\n",
      "[528,     1] loss: 0.52281761\n",
      "[529,     1] loss: 0.52024335\n",
      "[530,     1] loss: 0.51768041\n",
      "[531,     1] loss: 0.51512796\n",
      "[532,     1] loss: 0.51258594\n",
      "[533,     1] loss: 0.51005411\n",
      "[534,     1] loss: 0.50753427\n",
      "[535,     1] loss: 0.50502026\n",
      "[536,     1] loss: 0.50252008\n",
      "[537,     1] loss: 0.50003058\n",
      "[538,     1] loss: 0.49755251\n",
      "[539,     1] loss: 0.49508503\n",
      "[540,     1] loss: 0.49262711\n",
      "[541,     1] loss: 0.49018338\n",
      "[542,     1] loss: 0.48774487\n",
      "[543,     1] loss: 0.48532054\n",
      "[544,     1] loss: 0.48290661\n",
      "[545,     1] loss: 0.48050472\n",
      "[546,     1] loss: 0.47811267\n",
      "[547,     1] loss: 0.47573113\n",
      "[548,     1] loss: 0.47336018\n",
      "[549,     1] loss: 0.47100040\n",
      "[550,     1] loss: 0.46865222\n",
      "[551,     1] loss: 0.46631467\n",
      "[552,     1] loss: 0.46399057\n",
      "[553,     1] loss: 0.46167064\n",
      "[554,     1] loss: 0.45936882\n",
      "[555,     1] loss: 0.45707619\n",
      "[556,     1] loss: 0.45479387\n",
      "[557,     1] loss: 0.45252311\n",
      "[558,     1] loss: 0.45026255\n",
      "[559,     1] loss: 0.44801328\n",
      "[560,     1] loss: 0.44577530\n",
      "[561,     1] loss: 0.44354838\n",
      "[562,     1] loss: 0.44133240\n",
      "[563,     1] loss: 0.43912664\n",
      "[564,     1] loss: 0.43693894\n",
      "[565,     1] loss: 0.43475181\n",
      "[566,     1] loss: 0.43258113\n",
      "[567,     1] loss: 0.43042177\n",
      "[568,     1] loss: 0.42827201\n",
      "[569,     1] loss: 0.42613372\n",
      "[570,     1] loss: 0.42400733\n",
      "[571,     1] loss: 0.42189050\n",
      "[572,     1] loss: 0.41978645\n",
      "[573,     1] loss: 0.41769111\n",
      "[574,     1] loss: 0.41560841\n",
      "[575,     1] loss: 0.41353464\n",
      "[576,     1] loss: 0.41147438\n",
      "[577,     1] loss: 0.40942445\n",
      "[578,     1] loss: 0.40738535\n",
      "[579,     1] loss: 0.40535671\n",
      "[580,     1] loss: 0.40333804\n",
      "[581,     1] loss: 0.40133017\n",
      "[582,     1] loss: 0.39933464\n",
      "[583,     1] loss: 0.39734906\n",
      "[584,     1] loss: 0.39537269\n",
      "[585,     1] loss: 0.39340809\n",
      "[586,     1] loss: 0.39145264\n",
      "[587,     1] loss: 0.38950881\n",
      "[588,     1] loss: 0.38757697\n",
      "[589,     1] loss: 0.38565716\n",
      "[590,     1] loss: 0.38374171\n",
      "[591,     1] loss: 0.38183942\n",
      "[592,     1] loss: 0.37994802\n",
      "[593,     1] loss: 0.37806743\n",
      "[594,     1] loss: 0.37619555\n",
      "[595,     1] loss: 0.37433439\n",
      "[596,     1] loss: 0.37248322\n",
      "[597,     1] loss: 0.37064508\n",
      "[598,     1] loss: 0.36881277\n",
      "[599,     1] loss: 0.36699161\n",
      "[600,     1] loss: 0.36518002\n",
      "[601,     1] loss: 0.36338013\n",
      "[602,     1] loss: 0.36158833\n",
      "[603,     1] loss: 0.35980678\n",
      "[604,     1] loss: 0.35803512\n",
      "[605,     1] loss: 0.35627297\n",
      "[606,     1] loss: 0.35451159\n",
      "[607,     1] loss: 0.35275698\n",
      "[608,     1] loss: 0.35101041\n",
      "[609,     1] loss: 0.34927213\n",
      "[610,     1] loss: 0.34754175\n",
      "[611,     1] loss: 0.34582096\n",
      "[612,     1] loss: 0.34410870\n",
      "[613,     1] loss: 0.34240529\n",
      "[614,     1] loss: 0.34071118\n",
      "[615,     1] loss: 0.33902615\n",
      "[616,     1] loss: 0.33735269\n",
      "[617,     1] loss: 0.33568320\n",
      "[618,     1] loss: 0.33402535\n",
      "[619,     1] loss: 0.33237693\n",
      "[620,     1] loss: 0.33073738\n",
      "[621,     1] loss: 0.32910648\n",
      "[622,     1] loss: 0.32748687\n",
      "[623,     1] loss: 0.32587284\n",
      "[624,     1] loss: 0.32426921\n",
      "[625,     1] loss: 0.32267383\n",
      "[626,     1] loss: 0.32108796\n",
      "[627,     1] loss: 0.31951079\n",
      "[628,     1] loss: 0.31794217\n",
      "[629,     1] loss: 0.31638253\n",
      "[630,     1] loss: 0.31483170\n",
      "[631,     1] loss: 0.31328741\n",
      "[632,     1] loss: 0.31175306\n",
      "[633,     1] loss: 0.31022730\n",
      "[634,     1] loss: 0.30870873\n",
      "[635,     1] loss: 0.30720049\n",
      "[636,     1] loss: 0.30569834\n",
      "[637,     1] loss: 0.30420467\n",
      "[638,     1] loss: 0.30271953\n",
      "[639,     1] loss: 0.30124190\n",
      "[640,     1] loss: 0.29977214\n",
      "[641,     1] loss: 0.29831082\n",
      "[642,     1] loss: 0.29685730\n",
      "[643,     1] loss: 0.29541108\n",
      "[644,     1] loss: 0.29397184\n",
      "[645,     1] loss: 0.29254073\n",
      "[646,     1] loss: 0.29111758\n",
      "[647,     1] loss: 0.28970325\n",
      "[648,     1] loss: 0.28829327\n",
      "[649,     1] loss: 0.28689200\n",
      "[650,     1] loss: 0.28549868\n",
      "[651,     1] loss: 0.28411174\n",
      "[652,     1] loss: 0.28273177\n",
      "[653,     1] loss: 0.28135970\n",
      "[654,     1] loss: 0.27999464\n",
      "[655,     1] loss: 0.27863568\n",
      "[656,     1] loss: 0.27728382\n",
      "[657,     1] loss: 0.27593881\n",
      "[658,     1] loss: 0.27460107\n",
      "[659,     1] loss: 0.27327016\n",
      "[660,     1] loss: 0.27194542\n",
      "[661,     1] loss: 0.27062741\n",
      "[662,     1] loss: 0.26931646\n",
      "[663,     1] loss: 0.26801184\n",
      "[664,     1] loss: 0.26671344\n",
      "[665,     1] loss: 0.26542175\n",
      "[666,     1] loss: 0.26413834\n",
      "[667,     1] loss: 0.26285881\n",
      "[668,     1] loss: 0.26158640\n",
      "[669,     1] loss: 0.26032135\n",
      "[670,     1] loss: 0.25906113\n",
      "[671,     1] loss: 0.25780851\n",
      "[672,     1] loss: 0.25656104\n",
      "[673,     1] loss: 0.25531971\n",
      "[674,     1] loss: 0.25408512\n",
      "[675,     1] loss: 0.25285673\n",
      "[676,     1] loss: 0.25163379\n",
      "[677,     1] loss: 0.25041658\n",
      "[678,     1] loss: 0.24920529\n",
      "[679,     1] loss: 0.24800047\n",
      "[680,     1] loss: 0.24680120\n",
      "[681,     1] loss: 0.24560773\n",
      "[682,     1] loss: 0.24441986\n",
      "[683,     1] loss: 0.24323739\n",
      "[684,     1] loss: 0.24206136\n",
      "[685,     1] loss: 0.24089058\n",
      "[686,     1] loss: 0.23972553\n",
      "[687,     1] loss: 0.23856616\n",
      "[688,     1] loss: 0.23741199\n",
      "[689,     1] loss: 0.23626342\n",
      "[690,     1] loss: 0.23512243\n",
      "[691,     1] loss: 0.23398317\n",
      "[692,     1] loss: 0.23285154\n",
      "[693,     1] loss: 0.23172547\n",
      "[694,     1] loss: 0.23060378\n",
      "[695,     1] loss: 0.22948758\n",
      "[696,     1] loss: 0.22837675\n",
      "[697,     1] loss: 0.22727129\n",
      "[698,     1] loss: 0.22617067\n",
      "[699,     1] loss: 0.22507492\n",
      "[700,     1] loss: 0.22398457\n",
      "[701,     1] loss: 0.22289911\n",
      "[702,     1] loss: 0.22181860\n",
      "[703,     1] loss: 0.22074303\n",
      "[704,     1] loss: 0.21967259\n",
      "[705,     1] loss: 0.21860707\n",
      "[706,     1] loss: 0.21754733\n",
      "[707,     1] loss: 0.21649227\n",
      "[708,     1] loss: 0.21544161\n",
      "[709,     1] loss: 0.21439625\n",
      "[710,     1] loss: 0.21335609\n",
      "[711,     1] loss: 0.21232067\n",
      "[712,     1] loss: 0.21128966\n",
      "[713,     1] loss: 0.21026358\n",
      "[714,     1] loss: 0.20924261\n",
      "[715,     1] loss: 0.20822617\n",
      "[716,     1] loss: 0.20721462\n",
      "[717,     1] loss: 0.20620763\n",
      "[718,     1] loss: 0.20520571\n",
      "[719,     1] loss: 0.20420897\n",
      "[720,     1] loss: 0.20321688\n",
      "[721,     1] loss: 0.20222890\n",
      "[722,     1] loss: 0.20124570\n",
      "[723,     1] loss: 0.20026757\n",
      "[724,     1] loss: 0.19929378\n",
      "[725,     1] loss: 0.19832490\n",
      "[726,     1] loss: 0.19736066\n",
      "[727,     1] loss: 0.19640091\n",
      "[728,     1] loss: 0.19544572\n",
      "[729,     1] loss: 0.19449450\n",
      "[730,     1] loss: 0.19354840\n",
      "[731,     1] loss: 0.19260715\n",
      "[732,     1] loss: 0.19167002\n",
      "[733,     1] loss: 0.19073695\n",
      "[734,     1] loss: 0.18980896\n",
      "[735,     1] loss: 0.18888552\n",
      "[736,     1] loss: 0.18796599\n",
      "[737,     1] loss: 0.18705106\n",
      "[738,     1] loss: 0.18614072\n",
      "[739,     1] loss: 0.18523504\n",
      "[740,     1] loss: 0.18433326\n",
      "[741,     1] loss: 0.18343621\n",
      "[742,     1] loss: 0.18254350\n",
      "[743,     1] loss: 0.18165512\n",
      "[744,     1] loss: 0.18077104\n",
      "[745,     1] loss: 0.17989108\n",
      "[746,     1] loss: 0.17901517\n",
      "[747,     1] loss: 0.17814428\n",
      "[748,     1] loss: 0.17727765\n",
      "[749,     1] loss: 0.17641498\n",
      "[750,     1] loss: 0.17555681\n",
      "[751,     1] loss: 0.17470284\n",
      "[752,     1] loss: 0.17385347\n",
      "[753,     1] loss: 0.17300774\n",
      "[754,     1] loss: 0.17216672\n",
      "[755,     1] loss: 0.17132990\n",
      "[756,     1] loss: 0.17049721\n",
      "[757,     1] loss: 0.16966876\n",
      "[758,     1] loss: 0.16884424\n",
      "[759,     1] loss: 0.16802394\n",
      "[760,     1] loss: 0.16720793\n",
      "[761,     1] loss: 0.16639590\n",
      "[762,     1] loss: 0.16558841\n",
      "[763,     1] loss: 0.16478463\n",
      "[764,     1] loss: 0.16398482\n",
      "[765,     1] loss: 0.16318969\n",
      "[766,     1] loss: 0.16239853\n",
      "[767,     1] loss: 0.16161095\n",
      "[768,     1] loss: 0.16082807\n",
      "[769,     1] loss: 0.16004905\n",
      "[770,     1] loss: 0.15927398\n",
      "[771,     1] loss: 0.15850297\n",
      "[772,     1] loss: 0.15773624\n",
      "[773,     1] loss: 0.15697335\n",
      "[774,     1] loss: 0.15621448\n",
      "[775,     1] loss: 0.15545963\n",
      "[776,     1] loss: 0.15470889\n",
      "[777,     1] loss: 0.15396184\n",
      "[778,     1] loss: 0.15321873\n",
      "[779,     1] loss: 0.15247984\n",
      "[780,     1] loss: 0.15174487\n",
      "[781,     1] loss: 0.15101352\n",
      "[782,     1] loss: 0.15028605\n",
      "[783,     1] loss: 0.14956270\n",
      "[784,     1] loss: 0.14884356\n",
      "[785,     1] loss: 0.14812814\n",
      "[786,     1] loss: 0.14741634\n",
      "[787,     1] loss: 0.14670862\n",
      "[788,     1] loss: 0.14600453\n",
      "[789,     1] loss: 0.14530441\n",
      "[790,     1] loss: 0.14460805\n",
      "[791,     1] loss: 0.14391525\n",
      "[792,     1] loss: 0.14322644\n",
      "[793,     1] loss: 0.14254148\n",
      "[794,     1] loss: 0.14185996\n",
      "[795,     1] loss: 0.14118274\n",
      "[796,     1] loss: 0.14050889\n",
      "[797,     1] loss: 0.13983899\n",
      "[798,     1] loss: 0.13917282\n",
      "[799,     1] loss: 0.13851023\n",
      "[800,     1] loss: 0.13785124\n",
      "[801,     1] loss: 0.13719581\n",
      "[802,     1] loss: 0.13654418\n",
      "[803,     1] loss: 0.13589638\n",
      "[804,     1] loss: 0.13525224\n",
      "[805,     1] loss: 0.13461137\n",
      "[806,     1] loss: 0.13397415\n",
      "[807,     1] loss: 0.13334069\n",
      "[808,     1] loss: 0.13271075\n",
      "[809,     1] loss: 0.13208441\n",
      "[810,     1] loss: 0.13146162\n",
      "[811,     1] loss: 0.13084222\n",
      "[812,     1] loss: 0.13022631\n",
      "[813,     1] loss: 0.12961392\n",
      "[814,     1] loss: 0.12900530\n",
      "[815,     1] loss: 0.12839995\n",
      "[816,     1] loss: 0.12779802\n",
      "[817,     1] loss: 0.12719966\n",
      "[818,     1] loss: 0.12660477\n",
      "[819,     1] loss: 0.12601313\n",
      "[820,     1] loss: 0.12542476\n",
      "[821,     1] loss: 0.12484007\n",
      "[822,     1] loss: 0.12425885\n",
      "[823,     1] loss: 0.12368058\n",
      "[824,     1] loss: 0.12310562\n",
      "[825,     1] loss: 0.12253405\n",
      "[826,     1] loss: 0.12196591\n",
      "[827,     1] loss: 0.12140111\n",
      "[828,     1] loss: 0.12083961\n",
      "[829,     1] loss: 0.12028097\n",
      "[830,     1] loss: 0.11972578\n",
      "[831,     1] loss: 0.11917403\n",
      "[832,     1] loss: 0.11862533\n",
      "[833,     1] loss: 0.11807975\n",
      "[834,     1] loss: 0.11753746\n",
      "[835,     1] loss: 0.11699821\n",
      "[836,     1] loss: 0.11646227\n",
      "[837,     1] loss: 0.11592915\n",
      "[838,     1] loss: 0.11539944\n",
      "[839,     1] loss: 0.11487278\n",
      "[840,     1] loss: 0.11434924\n",
      "[841,     1] loss: 0.11382864\n",
      "[842,     1] loss: 0.11331101\n",
      "[843,     1] loss: 0.11279652\n",
      "[844,     1] loss: 0.11228494\n",
      "[845,     1] loss: 0.11177649\n",
      "[846,     1] loss: 0.11127066\n",
      "[847,     1] loss: 0.11076795\n",
      "[848,     1] loss: 0.11026847\n",
      "[849,     1] loss: 0.10977156\n",
      "[850,     1] loss: 0.10927767\n",
      "[851,     1] loss: 0.10878677\n",
      "[852,     1] loss: 0.10829890\n",
      "[853,     1] loss: 0.10781363\n",
      "[854,     1] loss: 0.10733116\n",
      "[855,     1] loss: 0.10685168\n",
      "[856,     1] loss: 0.10637492\n",
      "[857,     1] loss: 0.10590084\n",
      "[858,     1] loss: 0.10542993\n",
      "[859,     1] loss: 0.10496149\n",
      "[860,     1] loss: 0.10449565\n",
      "[861,     1] loss: 0.10403273\n",
      "[862,     1] loss: 0.10357257\n",
      "[863,     1] loss: 0.10311496\n",
      "[864,     1] loss: 0.10265997\n",
      "[865,     1] loss: 0.10220782\n",
      "[866,     1] loss: 0.10175827\n",
      "[867,     1] loss: 0.10131118\n",
      "[868,     1] loss: 0.10086696\n",
      "[869,     1] loss: 0.10042530\n",
      "[870,     1] loss: 0.09998616\n",
      "[871,     1] loss: 0.09954951\n",
      "[872,     1] loss: 0.09911567\n",
      "[873,     1] loss: 0.09868415\n",
      "[874,     1] loss: 0.09825517\n",
      "[875,     1] loss: 0.09782870\n",
      "[876,     1] loss: 0.09740452\n",
      "[877,     1] loss: 0.09698301\n",
      "[878,     1] loss: 0.09656394\n",
      "[879,     1] loss: 0.09614711\n",
      "[880,     1] loss: 0.09573295\n",
      "[881,     1] loss: 0.09532128\n",
      "[882,     1] loss: 0.09491191\n",
      "[883,     1] loss: 0.09450489\n",
      "[884,     1] loss: 0.09410021\n",
      "[885,     1] loss: 0.09369785\n",
      "[886,     1] loss: 0.09329794\n",
      "[887,     1] loss: 0.09290020\n",
      "[888,     1] loss: 0.09250475\n",
      "[889,     1] loss: 0.09211147\n",
      "[890,     1] loss: 0.09172059\n",
      "[891,     1] loss: 0.09133214\n",
      "[892,     1] loss: 0.09094577\n",
      "[893,     1] loss: 0.09056142\n",
      "[894,     1] loss: 0.09017963\n",
      "[895,     1] loss: 0.08979987\n",
      "[896,     1] loss: 0.08942250\n",
      "[897,     1] loss: 0.08904706\n",
      "[898,     1] loss: 0.08867373\n",
      "[899,     1] loss: 0.08830258\n",
      "[900,     1] loss: 0.08793361\n",
      "[901,     1] loss: 0.08756685\n",
      "[902,     1] loss: 0.08720202\n",
      "[903,     1] loss: 0.08683917\n",
      "[904,     1] loss: 0.08647866\n",
      "[905,     1] loss: 0.08611998\n",
      "[906,     1] loss: 0.08576360\n",
      "[907,     1] loss: 0.08540905\n",
      "[908,     1] loss: 0.08505651\n",
      "[909,     1] loss: 0.08470593\n",
      "[910,     1] loss: 0.08435743\n",
      "[911,     1] loss: 0.08401103\n",
      "[912,     1] loss: 0.08366634\n",
      "[913,     1] loss: 0.08332357\n",
      "[914,     1] loss: 0.08298309\n",
      "[915,     1] loss: 0.08264428\n",
      "[916,     1] loss: 0.08230735\n",
      "[917,     1] loss: 0.08197235\n",
      "[918,     1] loss: 0.08163929\n",
      "[919,     1] loss: 0.08130795\n",
      "[920,     1] loss: 0.08097843\n",
      "[921,     1] loss: 0.08065088\n",
      "[922,     1] loss: 0.08032509\n",
      "[923,     1] loss: 0.08000119\n",
      "[924,     1] loss: 0.07967923\n",
      "[925,     1] loss: 0.07935876\n",
      "[926,     1] loss: 0.07904027\n",
      "[927,     1] loss: 0.07872355\n",
      "[928,     1] loss: 0.07840852\n",
      "[929,     1] loss: 0.07809529\n",
      "[930,     1] loss: 0.07778363\n",
      "[931,     1] loss: 0.07747380\n",
      "[932,     1] loss: 0.07716561\n",
      "[933,     1] loss: 0.07685928\n",
      "[934,     1] loss: 0.07655451\n",
      "[935,     1] loss: 0.07625129\n",
      "[936,     1] loss: 0.07594986\n",
      "[937,     1] loss: 0.07565024\n",
      "[938,     1] loss: 0.07535134\n",
      "[939,     1] loss: 0.07505386\n",
      "[940,     1] loss: 0.07475779\n",
      "[941,     1] loss: 0.07446328\n",
      "[942,     1] loss: 0.07417013\n",
      "[943,     1] loss: 0.07387875\n",
      "[944,     1] loss: 0.07358892\n",
      "[945,     1] loss: 0.07330058\n",
      "[946,     1] loss: 0.07301390\n",
      "[947,     1] loss: 0.07272882\n",
      "[948,     1] loss: 0.07244505\n",
      "[949,     1] loss: 0.07216295\n",
      "[950,     1] loss: 0.07188237\n",
      "[951,     1] loss: 0.07160343\n",
      "[952,     1] loss: 0.07132581\n",
      "[953,     1] loss: 0.07104976\n",
      "[954,     1] loss: 0.07077517\n",
      "[955,     1] loss: 0.07050215\n",
      "[956,     1] loss: 0.07023048\n",
      "[957,     1] loss: 0.06996040\n",
      "[958,     1] loss: 0.06969182\n",
      "[959,     1] loss: 0.06942469\n",
      "[960,     1] loss: 0.06915889\n",
      "[961,     1] loss: 0.06889448\n",
      "[962,     1] loss: 0.06863156\n",
      "[963,     1] loss: 0.06837001\n",
      "[964,     1] loss: 0.06810987\n",
      "[965,     1] loss: 0.06785116\n",
      "[966,     1] loss: 0.06759375\n",
      "[967,     1] loss: 0.06733785\n",
      "[968,     1] loss: 0.06708330\n",
      "[969,     1] loss: 0.06683008\n",
      "[970,     1] loss: 0.06657805\n",
      "[971,     1] loss: 0.06632759\n",
      "[972,     1] loss: 0.06607831\n",
      "[973,     1] loss: 0.06583042\n",
      "[974,     1] loss: 0.06558372\n",
      "[975,     1] loss: 0.06533840\n",
      "[976,     1] loss: 0.06509437\n",
      "[977,     1] loss: 0.06485163\n",
      "[978,     1] loss: 0.06461008\n",
      "[979,     1] loss: 0.06436982\n",
      "[980,     1] loss: 0.06413089\n",
      "[981,     1] loss: 0.06389315\n",
      "[982,     1] loss: 0.06365661\n",
      "[983,     1] loss: 0.06342139\n",
      "[984,     1] loss: 0.06318729\n",
      "[985,     1] loss: 0.06295450\n",
      "[986,     1] loss: 0.06272287\n",
      "[987,     1] loss: 0.06249236\n",
      "[988,     1] loss: 0.06226315\n",
      "[989,     1] loss: 0.06203498\n",
      "[990,     1] loss: 0.06180794\n",
      "[991,     1] loss: 0.06158231\n",
      "[992,     1] loss: 0.06135778\n",
      "[993,     1] loss: 0.06113430\n",
      "[994,     1] loss: 0.06091198\n",
      "[995,     1] loss: 0.06069058\n",
      "[996,     1] loss: 0.06047053\n",
      "[997,     1] loss: 0.06025165\n",
      "[998,     1] loss: 0.06003391\n",
      "[999,     1] loss: 0.05981715\n",
      "[1000,     1] loss: 0.05960137\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "batch_size = 10\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(list(reward_net.parameters()), lr=0.001)\n",
    "\n",
    "losses = []\n",
    "\n",
    "idxs = np.array(range(len(states1)))\n",
    "num_batches = len(idxs) // batch_size\n",
    "\n",
    "# Train the model with regular SGD\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "   \n",
    "        t_states1 = torch.Tensor(states1).float().to(device)\n",
    "        t_states2 = torch.Tensor(states2).float().to(device)\n",
    "        t_prefs = torch.Tensor(prefs).float().to(device).unsqueeze(1)\n",
    "        t_features1a = torch.Tensor(features1a).float().to(device)\n",
    "        t_features1b = torch.Tensor(features1b).float().to(device)\n",
    "        t_prefs_feature1 = torch.Tensor(prefs_feature1).float().to(device).unsqueeze(1)\n",
    "        t_features2a = torch.Tensor(features2a).float().to(device)\n",
    "        t_features2b = torch.Tensor(features2b).float().to(device)\n",
    "        t_prefs_feature2 = torch.Tensor(prefs_feature2).float().to(device).unsqueeze(1)\n",
    "        t_features3a = torch.Tensor(features3a).float().to(device)\n",
    "        t_features3b = torch.Tensor(features3b).float().to(device)\n",
    "        t_prefs_feature3 = torch.Tensor(prefs_feature3).float().to(device).unsqueeze(1)\n",
    "        t_features4a = torch.Tensor(features4a).float().to(device)\n",
    "        t_features4b = torch.Tensor(features4b).float().to(device)\n",
    "        t_prefs_feature4 = torch.Tensor(prefs_feature4).float().to(device).unsqueeze(1)\n",
    "        t_features5a = torch.Tensor(features5a).float().to(device)\n",
    "        t_features5b = torch.Tensor(features5b).float().to(device)\n",
    "        t_prefs_feature5 = torch.Tensor(prefs_feature5).float().to(device).unsqueeze(1)\n",
    "        t_features6a = torch.Tensor(features6a).float().to(device)\n",
    "        t_features6b = torch.Tensor(features6b).float().to(device)\n",
    "        t_prefs_feature6 = torch.Tensor(prefs_feature6).float().to(device).unsqueeze(1)\n",
    "        pred_prefs_feature1, pred_prefs_feature2, pred_prefs_feature3, pred_prefs_feature4, pred_prefs_feature5, pred_prefs_feature6, pred_prefs = reward_net(t_features1a, t_features1b, t_features2a, t_features2b, t_features3a, t_features3b, \n",
    "                                                                                                                           t_features4a, t_features4b, t_features5a, t_features5b, t_features6a, t_features6b, \n",
    "                                                                                                                           t_states1, t_states2)\n",
    "        feature1_loss = criterion(pred_prefs_feature1, t_prefs_feature1)\n",
    "        feature2_loss = criterion(pred_prefs_feature2, t_prefs_feature2)\n",
    "        feature3_loss = criterion(pred_prefs_feature3, t_prefs_feature3)\n",
    "        feature4_loss = criterion(pred_prefs_feature4, t_prefs_feature4)\n",
    "        feature5_loss = criterion(pred_prefs_feature5, t_prefs_feature5)\n",
    "        feature6_loss = criterion(pred_prefs_feature6, t_prefs_feature6)\n",
    "        state_loss = criterion(pred_prefs, t_prefs)\n",
    "        loss = feature1_loss + feature2_loss + feature3_loss + feature4_loss + feature5_loss + feature6_loss + state_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        losses.append(loss.item())\n",
    "\n",
    "torch.save(reward_net, 'reward_network.pt')\n",
    "print('Finished Training')\n",
    "plt.plot(losses)\n",
    "plt.savefig('losses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8d58003-2c9e-43d4-a766-356a8e37ebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct: 0.900000 \n"
     ]
    }
   ],
   "source": [
    "reward_net = torch.load('reward_network.pt')\n",
    "reward_net.eval()\n",
    "\n",
    "import csv\n",
    "with open('data/test_rewards.csv') as file_obj:\n",
    "    reader_obj = csv.reader(file_obj)\n",
    "\n",
    "    states1, states2, prefs = [], [], []\n",
    "    features1a, features1b, prefs_feature1 = [], [], []\n",
    "    features2a, features2b, prefs_feature2 = [], [], []\n",
    "    features3a, features3b, prefs_feature3 = [], [], []\n",
    "    features4a, features4b, prefs_feature4 = [], [], []\n",
    "    features5a, features5b, prefs_feature5 = [], [], []\n",
    "    features6a, features6b, prefs_feature6 = [], [], []\n",
    "    for row in reader_obj:\n",
    "        states1.append(row[0:18])\n",
    "        states2.append(row[19:37])\n",
    "        prefs.append(row[38])\n",
    "        features1a.append(row[0:3])\n",
    "        features1b.append(row[19:22])\n",
    "        prefs_feature1.append(row[39])\n",
    "        features2a.append(row[3:6])\n",
    "        features2b.append(row[22:25])\n",
    "        prefs_feature2.append(row[40])\n",
    "        features3a.append(row[6:9])\n",
    "        features3b.append(row[25:28])\n",
    "        prefs_feature3.append(row[41])\n",
    "        features4a.append(row[9:12])\n",
    "        features4b.append(row[28:31])\n",
    "        prefs_feature4.append(row[42])\n",
    "        features5a.append(row[12:15])\n",
    "        features5b.append(row[31:34])\n",
    "        prefs_feature5.append(row[43])\n",
    "        features6a.append(row[15:18])\n",
    "        features6b.append(row[34:37])\n",
    "        prefs_feature6.append(row[44])\n",
    "    states1 = np.array(states1,dtype=int)\n",
    "    states2 = np.array(states2,dtype=int)\n",
    "    prefs = np.array(prefs,dtype=int)\n",
    "    features1a = np.array(features1a,dtype=int)\n",
    "    features1b = np.array(features1b,dtype=int)\n",
    "    prefs_feature1 = np.array(prefs_feature1,dtype=int)\n",
    "    features2a = np.array(features2a,dtype=int)\n",
    "    features2b = np.array(features2b,dtype=int)\n",
    "    prefs_feature2 = np.array(prefs_feature2,dtype=int)\n",
    "    features3a = np.array(features3a,dtype=int)\n",
    "    features3b = np.array(features3b,dtype=int)\n",
    "    prefs_feature3 = np.array(prefs_feature3,dtype=int)\n",
    "    features4a = np.array(features4a,dtype=int)\n",
    "    features4b = np.array(features4b,dtype=int)\n",
    "    prefs_feature4 = np.array(prefs_feature4,dtype=int)\n",
    "    features5a = np.array(features5a,dtype=int)\n",
    "    features5b = np.array(features5b,dtype=int)\n",
    "    prefs_feature5 = np.array(prefs_feature5,dtype=int)\n",
    "    features6a = np.array(features6a,dtype=int)\n",
    "    features6b = np.array(features6b,dtype=int)\n",
    "    prefs_feature6 = np.array(prefs_feature6,dtype=int)\n",
    "\n",
    "num_correct = 0.0\n",
    "for i in range(len(states1)):\n",
    "    state1 = torch.Tensor(states1[i]).to(device)\n",
    "    state2 = torch.Tensor(states2[i]).to(device)\n",
    "    feature1a = torch.Tensor(features1a[i]).to(device)\n",
    "    feature1b = torch.Tensor(features1b[i]).to(device)\n",
    "    feature2a = torch.Tensor(features2a[i]).to(device)\n",
    "    feature2b = torch.Tensor(features2b[i]).to(device)\n",
    "    feature3a = torch.Tensor(features3a[i]).to(device)\n",
    "    feature3b = torch.Tensor(features3b[i]).to(device)\n",
    "    feature4a = torch.Tensor(features4a[i]).to(device)\n",
    "    feature4b = torch.Tensor(features4b[i]).to(device)\n",
    "    feature5a = torch.Tensor(features5a[i]).to(device)\n",
    "    feature5b = torch.Tensor(features5b[i]).to(device)\n",
    "    feature6a = torch.Tensor(features6a[i]).to(device)\n",
    "    feature6b = torch.Tensor(features6b[i]).to(device)\n",
    "    pref_feature1, pref_feature2, pref_feature3, pref_feature4, pref_feature5, pref_feature6, pred_pref = reward_net(feature1a, feature1b, feature2a, feature2b, feature3a, feature3b, \n",
    "                                                                                                                feature4a, feature4b, feature5a, feature5b, feature6a, feature6b, \n",
    "                                                                                                                state1, state2)\n",
    "    pred_pref = torch.sigmoid(pred_pref).cpu().detach().numpy()[0]\n",
    "    if pred_pref > 0.5 and prefs[i] == 1:\n",
    "        num_correct+=1\n",
    "    elif pred_pref <= 0.5 and prefs[i] == 0:\n",
    "        num_correct+=1\n",
    "\n",
    "accuracy = num_correct / len(states1)\n",
    "print(\"Percent correct: %f \" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06a999-fb81-4a24-86d9-a046bdbccee5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "feature-preference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
