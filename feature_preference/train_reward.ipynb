{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fb9baee-b622-4c37-8707-0949e11ed13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, unicode_literals\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import distributions as pyd\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e205b6ac-00b6-4ca4-a56c-8af2711f4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom weight init for Conv2D and Linear layers\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.orthogonal_(m.weight.data)\n",
    "        if hasattr(m.bias, \"data\"):\n",
    "            m.bias.data.fill_(0.0)\n",
    "\n",
    "class RewardNet(nn.Module):\n",
    "    def __init__(\n",
    "        self, state_dim, output_mod=None):\n",
    "        super().__init__()\n",
    "        self.reward = nn.Sequential(\n",
    "            nn.Linear(state_dim, state_dim), nn.ReLU(),\n",
    "            nn.Linear(state_dim, state_dim), nn.ReLU(),\n",
    "            nn.Linear(state_dim, 1)\n",
    "        )\n",
    "        self.pref = nn.Linear(2, 1)\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, state1, state2):\n",
    "        r1 = self.reward(state1)\n",
    "        r2 = self.reward(state2)\n",
    "        comp = torch.squeeze(torch.stack([r1,r2], dim=1))\n",
    "        pref = self.pref(comp)\n",
    "        return r1, r2, pref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dd635ce5-260d-4f51-a206-0eabf9823d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RewardNet(\n",
       "  (reward): Sequential(\n",
       "    (0): Linear(in_features=18, out_features=18, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=18, out_features=18, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=18, out_features=1, bias=True)\n",
       "  )\n",
       "  (pref): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim = 18\n",
    "\n",
    "reward_net = RewardNet(state_dim)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "reward_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "441683da-6be2-4173-a7bc-dab41edc92b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('data/train_rewards500.csv') as file_obj:\n",
    "    reader_obj = csv.reader(file_obj)\n",
    "\n",
    "    states1 = []\n",
    "    states2 = []\n",
    "    prefs = []\n",
    "    for row in reader_obj:\n",
    "        states1.append(row[0:18])\n",
    "        states2.append(row[19:37])\n",
    "        prefs.append(row[38])\n",
    "    states1 = np.array(states1,dtype=int)\n",
    "    states2 = np.array(states2,dtype=int)\n",
    "    prefs = np.array(prefs,dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "66c6bce4-dfac-459e-9188-a428808dd0e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.41459787\n",
      "[2,     1] loss: 0.30287713\n",
      "[3,     1] loss: 0.22525656\n",
      "[4,     1] loss: 0.16480444\n",
      "[5,     1] loss: 0.11917467\n",
      "[6,     1] loss: 0.08673229\n",
      "[7,     1] loss: 0.06378244\n",
      "[8,     1] loss: 0.04818951\n",
      "[9,     1] loss: 0.03702718\n",
      "[10,     1] loss: 0.02888124\n",
      "[11,     1] loss: 0.02292932\n",
      "[12,     1] loss: 0.01857777\n",
      "[13,     1] loss: 0.01535728\n",
      "[14,     1] loss: 0.01292377\n",
      "[15,     1] loss: 0.01109276\n",
      "[16,     1] loss: 0.00969965\n",
      "[17,     1] loss: 0.00860281\n",
      "[18,     1] loss: 0.00770039\n",
      "[19,     1] loss: 0.00695321\n",
      "[20,     1] loss: 0.00632483\n",
      "[21,     1] loss: 0.00578869\n",
      "[22,     1] loss: 0.00532857\n",
      "[23,     1] loss: 0.00492785\n",
      "[24,     1] loss: 0.00457369\n",
      "[25,     1] loss: 0.00425112\n",
      "[26,     1] loss: 0.00395944\n",
      "[27,     1] loss: 0.00369816\n",
      "[28,     1] loss: 0.00346514\n",
      "[29,     1] loss: 0.00325741\n",
      "[30,     1] loss: 0.00306991\n",
      "[31,     1] loss: 0.00290067\n",
      "[32,     1] loss: 0.00274767\n",
      "[33,     1] loss: 0.00260905\n",
      "[34,     1] loss: 0.00248014\n",
      "[35,     1] loss: 0.00235736\n",
      "[36,     1] loss: 0.00224337\n",
      "[37,     1] loss: 0.00213850\n",
      "[38,     1] loss: 0.00204299\n",
      "[39,     1] loss: 0.00195450\n",
      "[40,     1] loss: 0.00187215\n",
      "[41,     1] loss: 0.00179489\n",
      "[42,     1] loss: 0.00172234\n",
      "[43,     1] loss: 0.00165387\n",
      "[44,     1] loss: 0.00158892\n",
      "[45,     1] loss: 0.00152776\n",
      "[46,     1] loss: 0.00147018\n",
      "[47,     1] loss: 0.00141584\n",
      "[48,     1] loss: 0.00136490\n",
      "[49,     1] loss: 0.00131663\n",
      "[50,     1] loss: 0.00127094\n",
      "[51,     1] loss: 0.00122770\n",
      "[52,     1] loss: 0.00118673\n",
      "[53,     1] loss: 0.00114786\n",
      "[54,     1] loss: 0.00111096\n",
      "[55,     1] loss: 0.00107587\n",
      "[56,     1] loss: 0.00104223\n",
      "[57,     1] loss: 0.00101001\n",
      "[58,     1] loss: 0.00097927\n",
      "[59,     1] loss: 0.00094996\n",
      "[60,     1] loss: 0.00092194\n",
      "[61,     1] loss: 0.00089472\n",
      "[62,     1] loss: 0.00086865\n",
      "[63,     1] loss: 0.00084389\n",
      "[64,     1] loss: 0.00082021\n",
      "[65,     1] loss: 0.00079771\n",
      "[66,     1] loss: 0.00077617\n",
      "[67,     1] loss: 0.00075553\n",
      "[68,     1] loss: 0.00073559\n",
      "[69,     1] loss: 0.00071635\n",
      "[70,     1] loss: 0.00069789\n",
      "[71,     1] loss: 0.00068018\n",
      "[72,     1] loss: 0.00066316\n",
      "[73,     1] loss: 0.00064679\n",
      "[74,     1] loss: 0.00063104\n",
      "[75,     1] loss: 0.00061589\n",
      "[76,     1] loss: 0.00060124\n",
      "[77,     1] loss: 0.00058712\n",
      "[78,     1] loss: 0.00057349\n",
      "[79,     1] loss: 0.00056029\n",
      "[80,     1] loss: 0.00054750\n",
      "[81,     1] loss: 0.00053513\n",
      "[82,     1] loss: 0.00052331\n",
      "[83,     1] loss: 0.00051172\n",
      "[84,     1] loss: 0.00050040\n",
      "[85,     1] loss: 0.00048942\n",
      "[86,     1] loss: 0.00047882\n",
      "[87,     1] loss: 0.00046874\n",
      "[88,     1] loss: 0.00045898\n",
      "[89,     1] loss: 0.00044954\n",
      "[90,     1] loss: 0.00044038\n",
      "[91,     1] loss: 0.00043151\n",
      "[92,     1] loss: 0.00042289\n",
      "[93,     1] loss: 0.00041453\n",
      "[94,     1] loss: 0.00040640\n",
      "[95,     1] loss: 0.00039849\n",
      "[96,     1] loss: 0.00039080\n",
      "[97,     1] loss: 0.00038333\n",
      "[98,     1] loss: 0.00037606\n",
      "[99,     1] loss: 0.00036900\n",
      "[100,     1] loss: 0.00036213\n",
      "[101,     1] loss: 0.00035540\n",
      "[102,     1] loss: 0.00034874\n",
      "[103,     1] loss: 0.00034225\n",
      "[104,     1] loss: 0.00033589\n",
      "[105,     1] loss: 0.00032967\n",
      "[106,     1] loss: 0.00032360\n",
      "[107,     1] loss: 0.00031771\n",
      "[108,     1] loss: 0.00031200\n",
      "[109,     1] loss: 0.00030645\n",
      "[110,     1] loss: 0.00030106\n",
      "[111,     1] loss: 0.00029581\n",
      "[112,     1] loss: 0.00029071\n",
      "[113,     1] loss: 0.00028573\n",
      "[114,     1] loss: 0.00028089\n",
      "[115,     1] loss: 0.00027616\n",
      "[116,     1] loss: 0.00027155\n",
      "[117,     1] loss: 0.00026704\n",
      "[118,     1] loss: 0.00026258\n",
      "[119,     1] loss: 0.00025821\n",
      "[120,     1] loss: 0.00025394\n",
      "[121,     1] loss: 0.00024977\n",
      "[122,     1] loss: 0.00024570\n",
      "[123,     1] loss: 0.00024172\n",
      "[124,     1] loss: 0.00023784\n",
      "[125,     1] loss: 0.00023406\n",
      "[126,     1] loss: 0.00023038\n",
      "[127,     1] loss: 0.00022678\n",
      "[128,     1] loss: 0.00022327\n",
      "[129,     1] loss: 0.00021983\n",
      "[130,     1] loss: 0.00021647\n",
      "[131,     1] loss: 0.00021316\n",
      "[132,     1] loss: 0.00020990\n",
      "[133,     1] loss: 0.00020670\n",
      "[134,     1] loss: 0.00020358\n",
      "[135,     1] loss: 0.00020053\n",
      "[136,     1] loss: 0.00019751\n",
      "[137,     1] loss: 0.00019456\n",
      "[138,     1] loss: 0.00019166\n",
      "[139,     1] loss: 0.00018883\n",
      "[140,     1] loss: 0.00018607\n",
      "[141,     1] loss: 0.00018336\n",
      "[142,     1] loss: 0.00018072\n",
      "[143,     1] loss: 0.00017813\n",
      "[144,     1] loss: 0.00017559\n",
      "[145,     1] loss: 0.00017310\n",
      "[146,     1] loss: 0.00017067\n",
      "[147,     1] loss: 0.00016830\n",
      "[148,     1] loss: 0.00016598\n",
      "[149,     1] loss: 0.00016371\n",
      "[150,     1] loss: 0.00016147\n",
      "[151,     1] loss: 0.00015928\n",
      "[152,     1] loss: 0.00015713\n",
      "[153,     1] loss: 0.00015502\n",
      "[154,     1] loss: 0.00015295\n",
      "[155,     1] loss: 0.00015092\n",
      "[156,     1] loss: 0.00014893\n",
      "[157,     1] loss: 0.00014697\n",
      "[158,     1] loss: 0.00014506\n",
      "[159,     1] loss: 0.00014317\n",
      "[160,     1] loss: 0.00014133\n",
      "[161,     1] loss: 0.00013951\n",
      "[162,     1] loss: 0.00013773\n",
      "[163,     1] loss: 0.00013598\n",
      "[164,     1] loss: 0.00013426\n",
      "[165,     1] loss: 0.00013258\n",
      "[166,     1] loss: 0.00013092\n",
      "[167,     1] loss: 0.00012929\n",
      "[168,     1] loss: 0.00012770\n",
      "[169,     1] loss: 0.00012609\n",
      "[170,     1] loss: 0.00012445\n",
      "[171,     1] loss: 0.00012282\n",
      "[172,     1] loss: 0.00012127\n",
      "[173,     1] loss: 0.00011974\n",
      "[174,     1] loss: 0.00011824\n",
      "[175,     1] loss: 0.00011678\n",
      "[176,     1] loss: 0.00011534\n",
      "[177,     1] loss: 0.00011394\n",
      "[178,     1] loss: 0.00011257\n",
      "[179,     1] loss: 0.00011122\n",
      "[180,     1] loss: 0.00010989\n",
      "[181,     1] loss: 0.00010859\n",
      "[182,     1] loss: 0.00010732\n",
      "[183,     1] loss: 0.00010606\n",
      "[184,     1] loss: 0.00010483\n",
      "[185,     1] loss: 0.00010362\n",
      "[186,     1] loss: 0.00010243\n",
      "[187,     1] loss: 0.00010126\n",
      "[188,     1] loss: 0.00010010\n",
      "[189,     1] loss: 0.00009897\n",
      "[190,     1] loss: 0.00009785\n",
      "[191,     1] loss: 0.00009675\n",
      "[192,     1] loss: 0.00009566\n",
      "[193,     1] loss: 0.00009460\n",
      "[194,     1] loss: 0.00009355\n",
      "[195,     1] loss: 0.00009251\n",
      "[196,     1] loss: 0.00009150\n",
      "[197,     1] loss: 0.00009050\n",
      "[198,     1] loss: 0.00008952\n",
      "[199,     1] loss: 0.00008855\n",
      "[200,     1] loss: 0.00008760\n",
      "[201,     1] loss: 0.00008666\n",
      "[202,     1] loss: 0.00008574\n",
      "[203,     1] loss: 0.00008483\n",
      "[204,     1] loss: 0.00008394\n",
      "[205,     1] loss: 0.00008305\n",
      "[206,     1] loss: 0.00008218\n",
      "[207,     1] loss: 0.00008132\n",
      "[208,     1] loss: 0.00008047\n",
      "[209,     1] loss: 0.00007964\n",
      "[210,     1] loss: 0.00007881\n",
      "[211,     1] loss: 0.00007800\n",
      "[212,     1] loss: 0.00007720\n",
      "[213,     1] loss: 0.00007641\n",
      "[214,     1] loss: 0.00007563\n",
      "[215,     1] loss: 0.00007485\n",
      "[216,     1] loss: 0.00007409\n",
      "[217,     1] loss: 0.00007333\n",
      "[218,     1] loss: 0.00007258\n",
      "[219,     1] loss: 0.00007184\n",
      "[220,     1] loss: 0.00007111\n",
      "[221,     1] loss: 0.00007040\n",
      "[222,     1] loss: 0.00006969\n",
      "[223,     1] loss: 0.00006899\n",
      "[224,     1] loss: 0.00006830\n",
      "[225,     1] loss: 0.00006762\n",
      "[226,     1] loss: 0.00006695\n",
      "[227,     1] loss: 0.00006628\n",
      "[228,     1] loss: 0.00006563\n",
      "[229,     1] loss: 0.00006498\n",
      "[230,     1] loss: 0.00006434\n",
      "[231,     1] loss: 0.00006372\n",
      "[232,     1] loss: 0.00006310\n",
      "[233,     1] loss: 0.00006249\n",
      "[234,     1] loss: 0.00006189\n",
      "[235,     1] loss: 0.00006130\n",
      "[236,     1] loss: 0.00006072\n",
      "[237,     1] loss: 0.00006014\n",
      "[238,     1] loss: 0.00005957\n",
      "[239,     1] loss: 0.00005901\n",
      "[240,     1] loss: 0.00005846\n",
      "[241,     1] loss: 0.00005791\n",
      "[242,     1] loss: 0.00005737\n",
      "[243,     1] loss: 0.00005683\n",
      "[244,     1] loss: 0.00005630\n",
      "[245,     1] loss: 0.00005578\n",
      "[246,     1] loss: 0.00005526\n",
      "[247,     1] loss: 0.00005475\n",
      "[248,     1] loss: 0.00005425\n",
      "[249,     1] loss: 0.00005375\n",
      "[250,     1] loss: 0.00005325\n",
      "[251,     1] loss: 0.00005277\n",
      "[252,     1] loss: 0.00005229\n",
      "[253,     1] loss: 0.00005181\n",
      "[254,     1] loss: 0.00005134\n",
      "[255,     1] loss: 0.00005088\n",
      "[256,     1] loss: 0.00005042\n",
      "[257,     1] loss: 0.00004996\n",
      "[258,     1] loss: 0.00004952\n",
      "[259,     1] loss: 0.00004908\n",
      "[260,     1] loss: 0.00004864\n",
      "[261,     1] loss: 0.00004821\n",
      "[262,     1] loss: 0.00004778\n",
      "[263,     1] loss: 0.00004736\n",
      "[264,     1] loss: 0.00004694\n",
      "[265,     1] loss: 0.00004653\n",
      "[266,     1] loss: 0.00004612\n",
      "[267,     1] loss: 0.00004572\n",
      "[268,     1] loss: 0.00004532\n",
      "[269,     1] loss: 0.00004493\n",
      "[270,     1] loss: 0.00004454\n",
      "[271,     1] loss: 0.00004415\n",
      "[272,     1] loss: 0.00004377\n",
      "[273,     1] loss: 0.00004340\n",
      "[274,     1] loss: 0.00004303\n",
      "[275,     1] loss: 0.00004266\n",
      "[276,     1] loss: 0.00004229\n",
      "[277,     1] loss: 0.00004193\n",
      "[278,     1] loss: 0.00004158\n",
      "[279,     1] loss: 0.00004123\n",
      "[280,     1] loss: 0.00004088\n",
      "[281,     1] loss: 0.00004053\n",
      "[282,     1] loss: 0.00004019\n",
      "[283,     1] loss: 0.00003986\n",
      "[284,     1] loss: 0.00003952\n",
      "[285,     1] loss: 0.00003919\n",
      "[286,     1] loss: 0.00003887\n",
      "[287,     1] loss: 0.00003854\n",
      "[288,     1] loss: 0.00003822\n",
      "[289,     1] loss: 0.00003791\n",
      "[290,     1] loss: 0.00003759\n",
      "[291,     1] loss: 0.00003728\n",
      "[292,     1] loss: 0.00003698\n",
      "[293,     1] loss: 0.00003667\n",
      "[294,     1] loss: 0.00003637\n",
      "[295,     1] loss: 0.00003608\n",
      "[296,     1] loss: 0.00003578\n",
      "[297,     1] loss: 0.00003549\n",
      "[298,     1] loss: 0.00003520\n",
      "[299,     1] loss: 0.00003491\n",
      "[300,     1] loss: 0.00003463\n",
      "[301,     1] loss: 0.00003435\n",
      "[302,     1] loss: 0.00003408\n",
      "[303,     1] loss: 0.00003380\n",
      "[304,     1] loss: 0.00003353\n",
      "[305,     1] loss: 0.00003326\n",
      "[306,     1] loss: 0.00003299\n",
      "[307,     1] loss: 0.00003273\n",
      "[308,     1] loss: 0.00003247\n",
      "[309,     1] loss: 0.00003221\n",
      "[310,     1] loss: 0.00003196\n",
      "[311,     1] loss: 0.00003170\n",
      "[312,     1] loss: 0.00003145\n",
      "[313,     1] loss: 0.00003120\n",
      "[314,     1] loss: 0.00003096\n",
      "[315,     1] loss: 0.00003072\n",
      "[316,     1] loss: 0.00003048\n",
      "[317,     1] loss: 0.00003024\n",
      "[318,     1] loss: 0.00003000\n",
      "[319,     1] loss: 0.00002977\n",
      "[320,     1] loss: 0.00002954\n",
      "[321,     1] loss: 0.00002931\n",
      "[322,     1] loss: 0.00002908\n",
      "[323,     1] loss: 0.00002886\n",
      "[324,     1] loss: 0.00002864\n",
      "[325,     1] loss: 0.00002841\n",
      "[326,     1] loss: 0.00002820\n",
      "[327,     1] loss: 0.00002798\n",
      "[328,     1] loss: 0.00002777\n",
      "[329,     1] loss: 0.00002755\n",
      "[330,     1] loss: 0.00002734\n",
      "[331,     1] loss: 0.00002714\n",
      "[332,     1] loss: 0.00002693\n",
      "[333,     1] loss: 0.00002673\n",
      "[334,     1] loss: 0.00002652\n",
      "[335,     1] loss: 0.00002632\n",
      "[336,     1] loss: 0.00002612\n",
      "[337,     1] loss: 0.00002593\n",
      "[338,     1] loss: 0.00002573\n",
      "[339,     1] loss: 0.00002554\n",
      "[340,     1] loss: 0.00002535\n",
      "[341,     1] loss: 0.00002516\n",
      "[342,     1] loss: 0.00002497\n",
      "[343,     1] loss: 0.00002478\n",
      "[344,     1] loss: 0.00002460\n",
      "[345,     1] loss: 0.00002441\n",
      "[346,     1] loss: 0.00002423\n",
      "[347,     1] loss: 0.00002405\n",
      "[348,     1] loss: 0.00002387\n",
      "[349,     1] loss: 0.00002369\n",
      "[350,     1] loss: 0.00002351\n",
      "[351,     1] loss: 0.00002334\n",
      "[352,     1] loss: 0.00002316\n",
      "[353,     1] loss: 0.00002299\n",
      "[354,     1] loss: 0.00002282\n",
      "[355,     1] loss: 0.00002265\n",
      "[356,     1] loss: 0.00002248\n",
      "[357,     1] loss: 0.00002232\n",
      "[358,     1] loss: 0.00002215\n",
      "[359,     1] loss: 0.00002199\n",
      "[360,     1] loss: 0.00002183\n",
      "[361,     1] loss: 0.00002167\n",
      "[362,     1] loss: 0.00002151\n",
      "[363,     1] loss: 0.00002135\n",
      "[364,     1] loss: 0.00002119\n",
      "[365,     1] loss: 0.00002104\n",
      "[366,     1] loss: 0.00002089\n",
      "[367,     1] loss: 0.00002073\n",
      "[368,     1] loss: 0.00002058\n",
      "[369,     1] loss: 0.00002043\n",
      "[370,     1] loss: 0.00002028\n",
      "[371,     1] loss: 0.00002014\n",
      "[372,     1] loss: 0.00001999\n",
      "[373,     1] loss: 0.00001984\n",
      "[374,     1] loss: 0.00001970\n",
      "[375,     1] loss: 0.00001956\n",
      "[376,     1] loss: 0.00001942\n",
      "[377,     1] loss: 0.00001928\n",
      "[378,     1] loss: 0.00001914\n",
      "[379,     1] loss: 0.00001900\n",
      "[380,     1] loss: 0.00001887\n",
      "[381,     1] loss: 0.00001873\n",
      "[382,     1] loss: 0.00001860\n",
      "[383,     1] loss: 0.00001846\n",
      "[384,     1] loss: 0.00001833\n",
      "[385,     1] loss: 0.00001820\n",
      "[386,     1] loss: 0.00001807\n",
      "[387,     1] loss: 0.00001795\n",
      "[388,     1] loss: 0.00001782\n",
      "[389,     1] loss: 0.00001769\n",
      "[390,     1] loss: 0.00001757\n",
      "[391,     1] loss: 0.00001745\n",
      "[392,     1] loss: 0.00001732\n",
      "[393,     1] loss: 0.00001720\n",
      "[394,     1] loss: 0.00001708\n",
      "[395,     1] loss: 0.00001696\n",
      "[396,     1] loss: 0.00001684\n",
      "[397,     1] loss: 0.00001672\n",
      "[398,     1] loss: 0.00001661\n",
      "[399,     1] loss: 0.00001649\n",
      "[400,     1] loss: 0.00001637\n",
      "[401,     1] loss: 0.00001626\n",
      "[402,     1] loss: 0.00001615\n",
      "[403,     1] loss: 0.00001604\n",
      "[404,     1] loss: 0.00001593\n",
      "[405,     1] loss: 0.00001581\n",
      "[406,     1] loss: 0.00001571\n",
      "[407,     1] loss: 0.00001560\n",
      "[408,     1] loss: 0.00001549\n",
      "[409,     1] loss: 0.00001538\n",
      "[410,     1] loss: 0.00001528\n",
      "[411,     1] loss: 0.00001517\n",
      "[412,     1] loss: 0.00001507\n",
      "[413,     1] loss: 0.00001496\n",
      "[414,     1] loss: 0.00001486\n",
      "[415,     1] loss: 0.00001476\n",
      "[416,     1] loss: 0.00001466\n",
      "[417,     1] loss: 0.00001456\n",
      "[418,     1] loss: 0.00001446\n",
      "[419,     1] loss: 0.00001436\n",
      "[420,     1] loss: 0.00001426\n",
      "[421,     1] loss: 0.00001417\n",
      "[422,     1] loss: 0.00001407\n",
      "[423,     1] loss: 0.00001397\n",
      "[424,     1] loss: 0.00001388\n",
      "[425,     1] loss: 0.00001379\n",
      "[426,     1] loss: 0.00001369\n",
      "[427,     1] loss: 0.00001360\n",
      "[428,     1] loss: 0.00001351\n",
      "[429,     1] loss: 0.00001342\n",
      "[430,     1] loss: 0.00001333\n",
      "[431,     1] loss: 0.00001324\n",
      "[432,     1] loss: 0.00001315\n",
      "[433,     1] loss: 0.00001306\n",
      "[434,     1] loss: 0.00001297\n",
      "[435,     1] loss: 0.00001288\n",
      "[436,     1] loss: 0.00001280\n",
      "[437,     1] loss: 0.00001271\n",
      "[438,     1] loss: 0.00001263\n",
      "[439,     1] loss: 0.00001254\n",
      "[440,     1] loss: 0.00001246\n",
      "[441,     1] loss: 0.00001238\n",
      "[442,     1] loss: 0.00001230\n",
      "[443,     1] loss: 0.00001221\n",
      "[444,     1] loss: 0.00001213\n",
      "[445,     1] loss: 0.00001205\n",
      "[446,     1] loss: 0.00001197\n",
      "[447,     1] loss: 0.00001189\n",
      "[448,     1] loss: 0.00001182\n",
      "[449,     1] loss: 0.00001174\n",
      "[450,     1] loss: 0.00001166\n",
      "[451,     1] loss: 0.00001158\n",
      "[452,     1] loss: 0.00001151\n",
      "[453,     1] loss: 0.00001143\n",
      "[454,     1] loss: 0.00001136\n",
      "[455,     1] loss: 0.00001128\n",
      "[456,     1] loss: 0.00001121\n",
      "[457,     1] loss: 0.00001114\n",
      "[458,     1] loss: 0.00001106\n",
      "[459,     1] loss: 0.00001099\n",
      "[460,     1] loss: 0.00001092\n",
      "[461,     1] loss: 0.00001085\n",
      "[462,     1] loss: 0.00001078\n",
      "[463,     1] loss: 0.00001071\n",
      "[464,     1] loss: 0.00001064\n",
      "[465,     1] loss: 0.00001057\n",
      "[466,     1] loss: 0.00001050\n",
      "[467,     1] loss: 0.00001043\n",
      "[468,     1] loss: 0.00001037\n",
      "[469,     1] loss: 0.00001030\n",
      "[470,     1] loss: 0.00001023\n",
      "[471,     1] loss: 0.00001017\n",
      "[472,     1] loss: 0.00001010\n",
      "[473,     1] loss: 0.00001004\n",
      "[474,     1] loss: 0.00000997\n",
      "[475,     1] loss: 0.00000991\n",
      "[476,     1] loss: 0.00000984\n",
      "[477,     1] loss: 0.00000978\n",
      "[478,     1] loss: 0.00000972\n",
      "[479,     1] loss: 0.00000965\n",
      "[480,     1] loss: 0.00000959\n",
      "[481,     1] loss: 0.00000953\n",
      "[482,     1] loss: 0.00000947\n",
      "[483,     1] loss: 0.00000941\n",
      "[484,     1] loss: 0.00000935\n",
      "[485,     1] loss: 0.00000929\n",
      "[486,     1] loss: 0.00000923\n",
      "[487,     1] loss: 0.00000917\n",
      "[488,     1] loss: 0.00000911\n",
      "[489,     1] loss: 0.00000906\n",
      "[490,     1] loss: 0.00000900\n",
      "[491,     1] loss: 0.00000894\n",
      "[492,     1] loss: 0.00000888\n",
      "[493,     1] loss: 0.00000883\n",
      "[494,     1] loss: 0.00000877\n",
      "[495,     1] loss: 0.00000872\n",
      "[496,     1] loss: 0.00000866\n",
      "[497,     1] loss: 0.00000861\n",
      "[498,     1] loss: 0.00000855\n",
      "[499,     1] loss: 0.00000850\n",
      "[500,     1] loss: 0.00000845\n",
      "[501,     1] loss: 0.00000839\n",
      "[502,     1] loss: 0.00000834\n",
      "[503,     1] loss: 0.00000829\n",
      "[504,     1] loss: 0.00000823\n",
      "[505,     1] loss: 0.00000818\n",
      "[506,     1] loss: 0.00000813\n",
      "[507,     1] loss: 0.00000808\n",
      "[508,     1] loss: 0.00000803\n",
      "[509,     1] loss: 0.00000798\n",
      "[510,     1] loss: 0.00000793\n",
      "[511,     1] loss: 0.00000788\n",
      "[512,     1] loss: 0.00000783\n",
      "[513,     1] loss: 0.00000778\n",
      "[514,     1] loss: 0.00000773\n",
      "[515,     1] loss: 0.00000768\n",
      "[516,     1] loss: 0.00000764\n",
      "[517,     1] loss: 0.00000759\n",
      "[518,     1] loss: 0.00000754\n",
      "[519,     1] loss: 0.00000749\n",
      "[520,     1] loss: 0.00000745\n",
      "[521,     1] loss: 0.00000740\n",
      "[522,     1] loss: 0.00000735\n",
      "[523,     1] loss: 0.00000731\n",
      "[524,     1] loss: 0.00000726\n",
      "[525,     1] loss: 0.00000721\n",
      "[526,     1] loss: 0.00000717\n",
      "[527,     1] loss: 0.00000713\n",
      "[528,     1] loss: 0.00000708\n",
      "[529,     1] loss: 0.00000704\n",
      "[530,     1] loss: 0.00000699\n",
      "[531,     1] loss: 0.00000695\n",
      "[532,     1] loss: 0.00000691\n",
      "[533,     1] loss: 0.00000686\n",
      "[534,     1] loss: 0.00000682\n",
      "[535,     1] loss: 0.00000678\n",
      "[536,     1] loss: 0.00000674\n",
      "[537,     1] loss: 0.00000669\n",
      "[538,     1] loss: 0.00000665\n",
      "[539,     1] loss: 0.00000661\n",
      "[540,     1] loss: 0.00000657\n",
      "[541,     1] loss: 0.00000653\n",
      "[542,     1] loss: 0.00000649\n",
      "[543,     1] loss: 0.00000645\n",
      "[544,     1] loss: 0.00000641\n",
      "[545,     1] loss: 0.00000637\n",
      "[546,     1] loss: 0.00000633\n",
      "[547,     1] loss: 0.00000629\n",
      "[548,     1] loss: 0.00000625\n",
      "[549,     1] loss: 0.00000621\n",
      "[550,     1] loss: 0.00000618\n",
      "[551,     1] loss: 0.00000614\n",
      "[552,     1] loss: 0.00000610\n",
      "[553,     1] loss: 0.00000606\n",
      "[554,     1] loss: 0.00000603\n",
      "[555,     1] loss: 0.00000599\n",
      "[556,     1] loss: 0.00000595\n",
      "[557,     1] loss: 0.00000591\n",
      "[558,     1] loss: 0.00000588\n",
      "[559,     1] loss: 0.00000584\n",
      "[560,     1] loss: 0.00000581\n",
      "[561,     1] loss: 0.00000577\n",
      "[562,     1] loss: 0.00000574\n",
      "[563,     1] loss: 0.00000570\n",
      "[564,     1] loss: 0.00000567\n",
      "[565,     1] loss: 0.00000563\n",
      "[566,     1] loss: 0.00000560\n",
      "[567,     1] loss: 0.00000556\n",
      "[568,     1] loss: 0.00000553\n",
      "[569,     1] loss: 0.00000549\n",
      "[570,     1] loss: 0.00000546\n",
      "[571,     1] loss: 0.00000543\n",
      "[572,     1] loss: 0.00000539\n",
      "[573,     1] loss: 0.00000536\n",
      "[574,     1] loss: 0.00000533\n",
      "[575,     1] loss: 0.00000530\n",
      "[576,     1] loss: 0.00000526\n",
      "[577,     1] loss: 0.00000523\n",
      "[578,     1] loss: 0.00000520\n",
      "[579,     1] loss: 0.00000517\n",
      "[580,     1] loss: 0.00000514\n",
      "[581,     1] loss: 0.00000511\n",
      "[582,     1] loss: 0.00000507\n",
      "[583,     1] loss: 0.00000504\n",
      "[584,     1] loss: 0.00000501\n",
      "[585,     1] loss: 0.00000498\n",
      "[586,     1] loss: 0.00000495\n",
      "[587,     1] loss: 0.00000492\n",
      "[588,     1] loss: 0.00000489\n",
      "[589,     1] loss: 0.00000486\n",
      "[590,     1] loss: 0.00000483\n",
      "[591,     1] loss: 0.00000480\n",
      "[592,     1] loss: 0.00000477\n",
      "[593,     1] loss: 0.00000475\n",
      "[594,     1] loss: 0.00000472\n",
      "[595,     1] loss: 0.00000469\n",
      "[596,     1] loss: 0.00000466\n",
      "[597,     1] loss: 0.00000463\n",
      "[598,     1] loss: 0.00000460\n",
      "[599,     1] loss: 0.00000458\n",
      "[600,     1] loss: 0.00000455\n",
      "[601,     1] loss: 0.00000452\n",
      "[602,     1] loss: 0.00000449\n",
      "[603,     1] loss: 0.00000447\n",
      "[604,     1] loss: 0.00000444\n",
      "[605,     1] loss: 0.00000441\n",
      "[606,     1] loss: 0.00000439\n",
      "[607,     1] loss: 0.00000436\n",
      "[608,     1] loss: 0.00000434\n",
      "[609,     1] loss: 0.00000431\n",
      "[610,     1] loss: 0.00000428\n",
      "[611,     1] loss: 0.00000426\n",
      "[612,     1] loss: 0.00000423\n",
      "[613,     1] loss: 0.00000421\n",
      "[614,     1] loss: 0.00000418\n",
      "[615,     1] loss: 0.00000416\n",
      "[616,     1] loss: 0.00000413\n",
      "[617,     1] loss: 0.00000411\n",
      "[618,     1] loss: 0.00000409\n",
      "[619,     1] loss: 0.00000406\n",
      "[620,     1] loss: 0.00000404\n",
      "[621,     1] loss: 0.00000401\n",
      "[622,     1] loss: 0.00000399\n",
      "[623,     1] loss: 0.00000396\n",
      "[624,     1] loss: 0.00000394\n",
      "[625,     1] loss: 0.00000392\n",
      "[626,     1] loss: 0.00000389\n",
      "[627,     1] loss: 0.00000387\n",
      "[628,     1] loss: 0.00000385\n",
      "[629,     1] loss: 0.00000382\n",
      "[630,     1] loss: 0.00000380\n",
      "[631,     1] loss: 0.00000378\n",
      "[632,     1] loss: 0.00000376\n",
      "[633,     1] loss: 0.00000373\n",
      "[634,     1] loss: 0.00000371\n",
      "[635,     1] loss: 0.00000369\n",
      "[636,     1] loss: 0.00000367\n",
      "[637,     1] loss: 0.00000364\n",
      "[638,     1] loss: 0.00000362\n",
      "[639,     1] loss: 0.00000360\n",
      "[640,     1] loss: 0.00000358\n",
      "[641,     1] loss: 0.00000356\n",
      "[642,     1] loss: 0.00000354\n",
      "[643,     1] loss: 0.00000352\n",
      "[644,     1] loss: 0.00000350\n",
      "[645,     1] loss: 0.00000348\n",
      "[646,     1] loss: 0.00000346\n",
      "[647,     1] loss: 0.00000344\n",
      "[648,     1] loss: 0.00000341\n",
      "[649,     1] loss: 0.00000339\n",
      "[650,     1] loss: 0.00000337\n",
      "[651,     1] loss: 0.00000335\n",
      "[652,     1] loss: 0.00000333\n",
      "[653,     1] loss: 0.00000332\n",
      "[654,     1] loss: 0.00000330\n",
      "[655,     1] loss: 0.00000328\n",
      "[656,     1] loss: 0.00000326\n",
      "[657,     1] loss: 0.00000324\n",
      "[658,     1] loss: 0.00000322\n",
      "[659,     1] loss: 0.00000320\n",
      "[660,     1] loss: 0.00000318\n",
      "[661,     1] loss: 0.00000316\n",
      "[662,     1] loss: 0.00000314\n",
      "[663,     1] loss: 0.00000312\n",
      "[664,     1] loss: 0.00000311\n",
      "[665,     1] loss: 0.00000309\n",
      "[666,     1] loss: 0.00000307\n",
      "[667,     1] loss: 0.00000305\n",
      "[668,     1] loss: 0.00000303\n",
      "[669,     1] loss: 0.00000302\n",
      "[670,     1] loss: 0.00000300\n",
      "[671,     1] loss: 0.00000298\n",
      "[672,     1] loss: 0.00000296\n",
      "[673,     1] loss: 0.00000294\n",
      "[674,     1] loss: 0.00000293\n",
      "[675,     1] loss: 0.00000291\n",
      "[676,     1] loss: 0.00000289\n",
      "[677,     1] loss: 0.00000288\n",
      "[678,     1] loss: 0.00000286\n",
      "[679,     1] loss: 0.00000284\n",
      "[680,     1] loss: 0.00000283\n",
      "[681,     1] loss: 0.00000281\n",
      "[682,     1] loss: 0.00000279\n",
      "[683,     1] loss: 0.00000278\n",
      "[684,     1] loss: 0.00000276\n",
      "[685,     1] loss: 0.00000275\n",
      "[686,     1] loss: 0.00000273\n",
      "[687,     1] loss: 0.00000271\n",
      "[688,     1] loss: 0.00000270\n",
      "[689,     1] loss: 0.00000268\n",
      "[690,     1] loss: 0.00000267\n",
      "[691,     1] loss: 0.00000265\n",
      "[692,     1] loss: 0.00000263\n",
      "[693,     1] loss: 0.00000262\n",
      "[694,     1] loss: 0.00000260\n",
      "[695,     1] loss: 0.00000259\n",
      "[696,     1] loss: 0.00000257\n",
      "[697,     1] loss: 0.00000256\n",
      "[698,     1] loss: 0.00000254\n",
      "[699,     1] loss: 0.00000253\n",
      "[700,     1] loss: 0.00000252\n",
      "[701,     1] loss: 0.00000250\n",
      "[702,     1] loss: 0.00000249\n",
      "[703,     1] loss: 0.00000247\n",
      "[704,     1] loss: 0.00000246\n",
      "[705,     1] loss: 0.00000244\n",
      "[706,     1] loss: 0.00000243\n",
      "[707,     1] loss: 0.00000242\n",
      "[708,     1] loss: 0.00000240\n",
      "[709,     1] loss: 0.00000239\n",
      "[710,     1] loss: 0.00000237\n",
      "[711,     1] loss: 0.00000236\n",
      "[712,     1] loss: 0.00000234\n",
      "[713,     1] loss: 0.00000233\n",
      "[714,     1] loss: 0.00000232\n",
      "[715,     1] loss: 0.00000231\n",
      "[716,     1] loss: 0.00000229\n",
      "[717,     1] loss: 0.00000228\n",
      "[718,     1] loss: 0.00000227\n",
      "[719,     1] loss: 0.00000225\n",
      "[720,     1] loss: 0.00000224\n",
      "[721,     1] loss: 0.00000223\n",
      "[722,     1] loss: 0.00000221\n",
      "[723,     1] loss: 0.00000220\n",
      "[724,     1] loss: 0.00000219\n",
      "[725,     1] loss: 0.00000218\n",
      "[726,     1] loss: 0.00000216\n",
      "[727,     1] loss: 0.00000215\n",
      "[728,     1] loss: 0.00000214\n",
      "[729,     1] loss: 0.00000213\n",
      "[730,     1] loss: 0.00000211\n",
      "[731,     1] loss: 0.00000210\n",
      "[732,     1] loss: 0.00000209\n",
      "[733,     1] loss: 0.00000208\n",
      "[734,     1] loss: 0.00000207\n",
      "[735,     1] loss: 0.00000206\n",
      "[736,     1] loss: 0.00000204\n",
      "[737,     1] loss: 0.00000203\n",
      "[738,     1] loss: 0.00000202\n",
      "[739,     1] loss: 0.00000201\n",
      "[740,     1] loss: 0.00000200\n",
      "[741,     1] loss: 0.00000199\n",
      "[742,     1] loss: 0.00000197\n",
      "[743,     1] loss: 0.00000196\n",
      "[744,     1] loss: 0.00000195\n",
      "[745,     1] loss: 0.00000194\n",
      "[746,     1] loss: 0.00000193\n",
      "[747,     1] loss: 0.00000192\n",
      "[748,     1] loss: 0.00000191\n",
      "[749,     1] loss: 0.00000189\n",
      "[750,     1] loss: 0.00000188\n",
      "Finished Training\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 750\n",
    "batch_size = 50\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(list(reward_net.parameters()), lr=0.001)\n",
    "\n",
    "losses = []\n",
    "\n",
    "idxs = np.array(range(len(states1)))\n",
    "num_batches = len(idxs) // batch_size\n",
    "\n",
    "# Train the model with regular SGD\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "   \n",
    "        t_states1 = torch.Tensor(states1).float().to(device)\n",
    "        t_states2 = torch.Tensor(states2).float().to(device)\n",
    "        t_prefs = torch.Tensor(prefs).float().to(device).unsqueeze(1)\n",
    "\n",
    "        pred_r1s, pred_r2s, pred_prefs = reward_net(t_states1, t_states2)\n",
    "        loss = criterion(pred_prefs, t_prefs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print('[%d, %5d] loss: %.8f' %\n",
    "                  (epoch + 1, i + 1, running_loss))\n",
    "            losses.append(running_loss)\n",
    "            running_loss = 0.0\n",
    "        losses.append(loss.item())\n",
    "\n",
    "torch.save(reward_net, 'reward_network.pt')\n",
    "print('Finished Training')\n",
    "plt.plot(losses)\n",
    "plt.savefig('losses.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18471169-b9b6-4099-99c6-00d46eba7376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct: 1.000000 \n"
     ]
    }
   ],
   "source": [
    "reward_net = torch.load('reward_network.pt')\n",
    "reward_net.eval()\n",
    "\n",
    "import csv\n",
    "with open('data/test_rewards.csv') as file_obj:\n",
    "    reader_obj = csv.reader(file_obj)\n",
    "\n",
    "    states1 = []\n",
    "    states2 = []\n",
    "    prefs = []\n",
    "    for row in reader_obj:\n",
    "        states1.append(row[0:18])\n",
    "        states2.append(row[19:37])\n",
    "        prefs.append(row[38])\n",
    "    states1 = np.array(states1,dtype=int)\n",
    "    states2 = np.array(states2,dtype=int)\n",
    "    prefs = np.array(prefs,dtype=int)\n",
    "\n",
    "num_correct = 0.0\n",
    "for n in range(len(states1)):\n",
    "    state1 = torch.Tensor(states1[n]).to(device)\n",
    "    state2 = torch.Tensor(states2[n]).to(device)\n",
    "    pred_r1, pred_r2, pred_pref = reward_net(state1, state2)\n",
    "    pred_r1 = torch.sigmoid(pred_r1)\n",
    "    pred_r2 = torch.sigmoid(pred_r2)\n",
    "    pred_pref = torch.sigmoid(pred_pref).cpu().detach().numpy()[0]\n",
    "    if pred_pref > 0.5 and prefs[n] == 1:\n",
    "        num_correct+=1\n",
    "    elif pred_pref <= 0.5 and prefs[n] == 0:\n",
    "        num_correct+=1\n",
    "\n",
    "accuracy = num_correct / len(states1)\n",
    "print(\"Percent correct: %f \" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf5a32db-b7f2-4442-b17b-693c1457ac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.2771e-20], grad_fn=<SigmoidBackward0>)\n",
      "tensor([4.7924e-18], grad_fn=<SigmoidBackward0>)\n",
      "0.9999869\n"
     ]
    }
   ],
   "source": [
    "print(pred_r1)\n",
    "print(pred_r2)\n",
    "print(pred_pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "47cf03fe-249d-4e6a-9097-38a18f64efd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0000], grad_fn=<SigmoidBackward0>)\n",
      "tensor([1.0000], grad_fn=<SigmoidBackward0>)\n",
      "tensor([0.2327], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "best_mushroom = torch.Tensor(np.array([1,0,0,1,0,0,1,0,0,1,0,0,0,0,1,1,0,0])).to(device)\n",
    "pred_r1, pred_r2, pred_pref = reward_net(best_mushroom,best_mushroom)\n",
    "print(torch.sigmoid(pred_r1))\n",
    "print(torch.sigmoid(pred_r2))\n",
    "print(torch.sigmoid(pred_pref))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf83b82-d48a-45a9-9d65-6e0194315b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
